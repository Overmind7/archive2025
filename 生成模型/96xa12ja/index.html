<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.21" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.143" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><link rel="icon" type="image/png" href="https://theme-plume.vuejs.press/favicon-32x32.png"><title>CLIP引导生成 | aurora</title><meta name="description" content=""><link rel="preload" href="/archive2025/assets/style-Nqw6QQQ5.css" as="style"><link rel="stylesheet" href="/archive2025/assets/style-Nqw6QQQ5.css"><link rel="modulepreload" href="/archive2025/assets/app-IQA0dJD3.js"><link rel="modulepreload" href="/archive2025/assets/index.html-BWzI-Skq.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-d90a7a26><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d5a8d0bc></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-d5a8d0bc> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-d90a7a26 data-v-e98a6132><div class="vp-navbar" vp-navbar data-v-e98a6132 data-v-2c31ea5e><div class="wrapper" data-v-2c31ea5e><div class="container" data-v-2c31ea5e><div class="title" data-v-2c31ea5e><div class="vp-navbar-title has-sidebar" data-v-2c31ea5e data-v-1a4f50af><a class="vp-link no-icon link title" href="/archive2025/" data-v-1a4f50af data-v-442a52aa><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="https://theme-plume.vuejs.press/plume.png" alt data-v-eda4b9bd><!--]--><!--[--><img class="vp-image light logo" style="" src="https://theme-plume.vuejs.press/plume.png" alt data-v-eda4b9bd><!--]--><!--]--><!--]--><span data-v-1a4f50af>aurora</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-2c31ea5e><div class="content-body" data-v-2c31ea5e><!--[--><!--]--><div class="vp-navbar-search search" data-v-2c31ea5e><div class="search-wrapper" data-v-97535d1e><!----><div id="local-search" data-v-97535d1e><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-97535d1e><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-2c31ea5e data-v-d43c1732><span id="main-nav-aria-label" class="visually-hidden" data-v-d43c1732>Main Navigation</span><!--[--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>首页</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>博客</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/tags/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>标签</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/archives/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>归档</span><!----><!--]--><!----></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-d43c1732 data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-86530b6c><span class="text" data-v-86530b6c><!----><!----><span data-v-86530b6c>笔记</span><!----><span class="vpi-chevron-down text-icon" data-v-86530b6c></span></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><div class="items" data-v-709dc2b1><!--[--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/demo/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 示例 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/SLAM/zc7d97ub/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> SLAM <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-group" data-v-709dc2b1 data-v-c497e9e3><p class="title" data-v-c497e9e3><!----><span data-v-c497e9e3>AI</span></p><!--[--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 生成模型 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E6%84%9F%E7%9F%A5/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 感知 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E6%95%B0%E6%8D%AE%E9%9B%86/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 数据集 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E5%88%86%E5%B1%82%E8%AF%AD%E4%B9%89/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 分层语义 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/VLN/" data-v-1ff1855f data-v-442a52aa><!--[--><!---->  VLN <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-2c31ea5e data-v-a295abf6><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-a295abf6 data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-2c31ea5e data-v-ad52545c data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="/" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-2c31ea5e data-v-652282fd data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-86530b6c><span class="vpi-more-horizontal icon" data-v-86530b6c></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><!----><!--[--><!--[--><!----><div class="group" data-v-652282fd><div class="item appearance" data-v-652282fd><p class="label" data-v-652282fd>外观</p><div class="appearance-action" data-v-652282fd><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-652282fd data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-652282fd><div class="item social-links" data-v-652282fd><div class="vp-social-links social-links-list" data-v-652282fd data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="/" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-2c31ea5e data-v-2b50024d><span class="container" data-v-2b50024d><span class="top" data-v-2b50024d></span><span class="middle" data-v-2b50024d></span><span class="bottom" data-v-2b50024d></span></span></button></div></div></div></div><div class="divider" data-v-2c31ea5e><div class="divider-line" data-v-2c31ea5e></div></div></div><!----></header><div class="vp-local-nav reached-top" data-v-d90a7a26 data-v-3944d8e8><button class="menu" aria-expanded="false" aria-controls="SidebarNav" data-v-3944d8e8><span class="vpi-align-left menu-icon" data-v-3944d8e8></span><span class="menu-text" data-v-3944d8e8>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-3944d8e8 data-v-4114a62c><button data-v-4114a62c>返回顶部</button><!----></div></div><aside class="vp-sidebar" vp-sidebar data-v-d90a7a26 data-v-3c61d592><div class="curtain" data-v-3c61d592></div><nav id="SidebarNav" class="nav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-3c61d592><span id="sidebar-aria-label" class="visually-hidden" data-v-3c61d592> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-473fd05b><section class="vp-sidebar-item sidebar-item level-0" data-v-473fd05b data-v-979dadab><!----><div data-v-979dadab data-v-979dadab><div class="items" data-v-979dadab><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>生成模型</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><div class="no-transition group" data-v-473fd05b><section class="vp-sidebar-item sidebar-item level-0 collapsible has-active" data-v-473fd05b data-v-979dadab><div class="item" role="button" tabindex="0" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><h2 class="text" data-v-979dadab><span data-v-979dadab>扩散模型</span><!----></h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-979dadab><span class="vpi-chevron-right caret-icon" data-v-979dadab></span></div></div><div data-v-979dadab data-v-979dadab><div class="items" data-v-979dadab><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/olqxk7qw/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>DDPM</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/jeaasjxx/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>DDIM</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/8deusd81/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>SDE</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/8deusd82/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>Score based Diffusion model</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/fmdki2js/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>条件扩散</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/2gu6h6qk/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>扩散模型与受控图像生成</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/96xa12ja/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>CLIP引导生成</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/l60xc115/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>StableDiffusion</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/9laka0pz/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>SR3</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/1po6vfar/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>Deblurring-via-Stochastic-Refinement</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/cc7ucksh/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>DiffSeg</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/umashxv4/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>PoseDiffusion</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><div class="no-transition group" data-v-473fd05b><section class="vp-sidebar-item sidebar-item level-0 collapsible collapsed" data-v-473fd05b data-v-979dadab><div class="item" role="button" tabindex="0" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><h2 class="text" data-v-979dadab><span data-v-979dadab>GAN</span><!----></h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-979dadab><span class="vpi-chevron-right caret-icon" data-v-979dadab></span></div></div><div style="display:none;" data-v-979dadab data-v-979dadab><div class="items" data-v-979dadab><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/g4otw67p/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>GAN生成对抗模型</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/rqcexhzb/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>StyleGAN</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/q4lwpm2b/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>DragGan</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><!--]--><!--[--><!--]--></nav></aside><!--[--><div id="VPContent" vp-content class="vp-content has-sidebar" data-v-d90a7a26 data-v-b2beaca7><div class="vp-doc-container has-sidebar has-aside" data-v-b2beaca7 data-v-a703f9d3><!--[--><!--]--><div class="container" data-v-a703f9d3><div class="aside" vp-outline data-v-a703f9d3><div class="aside-curtain" data-v-a703f9d3></div><div class="aside-container" data-v-a703f9d3><div class="aside-content" data-v-a703f9d3><div class="vp-doc-aside" data-v-a703f9d3 data-v-5976474c><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-5976474c data-v-aa56eba0><div class="content" data-v-aa56eba0><div class="outline-marker" data-v-aa56eba0></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-aa56eba0><span data-v-aa56eba0>此页内容</span><span class="vpi-print icon" data-v-aa56eba0></span></div><ul class="root" data-v-aa56eba0 data-v-3e6b023c><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-5976474c></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-a703f9d3><div class="content-container" data-v-a703f9d3><!--[--><!--]--><main class="main" data-v-a703f9d3><nav class="vp-breadcrumb" data-v-a703f9d3 data-v-1ae4ad7a><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-1ae4ad7a><!--[--><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><a class="vp-link no-icon link breadcrumb" href="/archive2025/" property="item" typeof="WebPage" data-v-1ae4ad7a data-v-442a52aa><!--[-->首页<!--]--><!----></a><span class="vpi-chevron-right" data-v-1ae4ad7a></span><meta property="name" content="首页" data-v-1ae4ad7a><meta property="position" content="1" data-v-1ae4ad7a></li><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><span class="vp-link no-icon breadcrumb" href="/archive2025/" property="item" typeof="WebPage" data-v-1ae4ad7a data-v-442a52aa><!--[-->扩散模型<!--]--><!----></span><span class="vpi-chevron-right" data-v-1ae4ad7a></span><meta property="name" content="扩散模型" data-v-1ae4ad7a><meta property="position" content="2" data-v-1ae4ad7a></li><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><a class="vp-link no-icon link breadcrumb current" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/96xa12ja/" property="item" typeof="WebPage" data-v-1ae4ad7a data-v-442a52aa><!--[-->CLIP引导生成<!--]--><!----></a><!----><meta property="name" content="CLIP引导生成" data-v-1ae4ad7a><meta property="position" content="3" data-v-1ae4ad7a></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-27be53cb>CLIP引导生成 <!----></h1><div class="vp-doc-meta" data-v-27be53cb><!--[--><!--]--><p class="reading-time" data-v-27be53cb><span class="vpi-books icon" data-v-27be53cb></span><span data-v-27be53cb>约 2203 字</span><span data-v-27be53cb>大约 7 分钟</span></p><!----><!--[--><!--]--><p class="create-time" data-v-27be53cb><span class="vpi-clock icon" data-v-27be53cb></span><span data-v-27be53cb>1984-01-24</span></p></div><!--]--><!--[--><!--]--><div class="_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B_96xa12ja_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-a703f9d3><!--[--><!--]--><div data-v-a703f9d3><blockquote><p>Learning Transferable Visual Models From Natural Language Supervision</p></blockquote><blockquote><p>26 Feb 2021</p><p><a href="https://openai.com/research/clip" target="_blank" rel="noopener noreferrer">CLIP: Connecting text and images (openai.com)</a></p><p><a href="https://inmeta.medium.com/clip-from-openai-what-is-it-and-how-you-can-try-it-out-yourself-6f9a870efe00" target="_blank" rel="noopener noreferrer">CLIP from OpenAI: what is it and how you can try it out yourself | by Inmeta | Medium</a></p></blockquote><h2 id="出发点" tabindex="-1"><a class="header-anchor" href="#出发点"><span>出发点</span></a></h2><ul><li>传统的图像分类模型无法对类别进行拓展，想要保证准确率只能从头开始训练，费时费力。</li><li>CLIP模型就可以用来解决这种问题，预训练后的模型就可以直接进行zero-shot</li></ul><h2 id="成果" tabindex="-1"><a class="header-anchor" href="#成果"><span>成果</span></a></h2><ul><li><strong>将图像和任意的文本联系起来</strong>，只需要简单地提供包含新类别的文本描述就可以使用该模型来识别新类别。</li><li>CLIP在完全不使用ImageNet中所有训练数据的前提下直接Zero-shot得到的结果与ResNet在128W ImageNet数据训练效果一致</li><li>CLIP使用4亿个配对的数据和文本来进行训练，不标注直接爬取（没有解决transformer训练所需数据量大的缺点）</li></ul><hr><h2 id="train" tabindex="-1"><a class="header-anchor" href="#train"><span>Train</span></a></h2><p>以一个batch size为N的输入为例，</p><ul><li>首先，N张图像和N个文本分别被各自模态的Encoder编码成高维向量。</li><li>然后，用它们的向量表示建立一个相似度矩阵（图中，I*T表示两模态向量的内积）。值得注意的是，在训练过程中，<strong>矩阵对角线上的内积是匹配图文的内积</strong>（即当前batch内，文本T1和图像I1是匹配的图文对，而文本T1和图像I2是不匹配的图文对）。我们知道内积越大，相似度越高，因此匹配的图文对的相似度（内积）必须高于同一行/列中其他图文对的相似度（内积）才合理。</li><li>于是，训练的目标可以看作是在进行对比，对比的目的是使同一行/列中匹配图文的内积尽可能大，不匹配图文的内积尽可能小。我们也可以用更通俗的方式来理解：每一行都是一个分类任务，给定一个输入图像I，预测匹配的那个文本是谁。同理，每一列都是一个分类任务：给定输入文本T，预测匹配的那张图像是谁。</li><li>在训练期间，Open AI使用了非常大规模的batch size（32768），这可以充分发挥这种对比训练的潜力。</li></ul><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230603105521598.png" alt="image-20230603105521598" style="zoom:50%;"><blockquote><p>标准图像模型联合训练图像特征提取器和线性分类器来预测某些标签，而 CLIP 联合训练图像编码器和文本编码器来预测一批（图像、文本）训练示例的正确配对。在测试时，学习的文本编码器通过嵌入目标数据集类的名称或描述来合成零样本线性分类器。</p></blockquote><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># image_encoder - ResNet or Vision Transformer</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># text_encoder  - CBOW or Text Transformer</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># I[n, h, w, c] - minibatch of aligned images</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># T[n, l]       - minibatch of aligned texts</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># W_i[d_i, d_e] - learned proj of image to embed</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># W_t[d_t, d_e] - learned proj of text to embed</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># t             - learned temperature parameter</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># 分别提取图像特征和文本特征</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">I_f </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> image_encoder</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">I</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> #[n, d_i]</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">T_f </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> text_encoder</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">T</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> #[n, d_t]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">I_e </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> l2_normalize</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">np</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dot</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">I_f</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> W_i</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">),</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> axis</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">T_e </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> l2_normalize</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">np</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dot</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">T_f</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> W_t</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">),</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> axis</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># 计算缩放的余弦相似度：[n, n]</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">logits </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> np</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dot</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">I_e</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> T_e</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">T</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> *</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> np</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">exp</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">t</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># 对称的对比学习损失：等价于N个类别的cross_entropy_loss</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">labels </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> np</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">arange</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">n</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> # 对角线元素的labels</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">loss_i </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> cross_entropy_loss</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">logits</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> labels</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> axis</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">loss_t </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> cross_entropy_loss</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">logits</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> labels</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> axis</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">loss </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> (</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">loss_i </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">+</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> loss_t</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">/</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">2</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="infer" tabindex="-1"><a class="header-anchor" href="#infer"><span>infer</span></a></h2><p>在推理过程中，使用者可以按照prompt（提示词）的格式自定义新文本。将新文本和图像送入CLIP模型后，通过内积值的大小来判断新文本和图像是否是匹配的。如下图所示，提示词是a photo of a {object}.，我们只需要将我们想判断的类别跟{object}进行替换即可。例如，我想判断这个图片是不是狗，我的新文本就是a photo of a dog.</p><p>经典的分类训练只关心模型是否可以正确预测图像的分类标签。如果模型预测成功了狗，那么它不在乎图像是一张狗的照片，还是一张狗的素描。而CLIP模型在大规模数据集上完成的训练，这使得CLIP模型还学习到了图像的各方面信息。</p><p>例如，<strong>CLIP模型对用于图像描述的单词很敏感</strong>。文本“a photo of a bird”、“a photo of a bird siting near bird feeder”或“an image of a bird”与相同的图像匹配产生的概率是不同的。</p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/1IrZpxiICRN-SXmpryhLNtA.png" alt="img" style="zoom:67%;"><h2 id="结果" tabindex="-1"><a class="header-anchor" href="#结果"><span>结果</span></a></h2><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230603114018949.png" alt="image-20230603114018949"></p><h1 id="clip损失引导生成" tabindex="-1"><a class="header-anchor" href="#clip损失引导生成"><span>CLIP损失引导生成</span></a></h1><h2 id="diffusioncllp" tabindex="-1"><a class="header-anchor" href="#diffusioncllp"><span>DiffusionCLlP</span></a></h2><blockquote><p><a href="https://paperswithcode.com/paper/diffusionclip-text-guided-image-manipulation" target="_blank" rel="noopener noreferrer">DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation | Papers With Code</a></p><p>CVPR 2022</p></blockquote><p>过使扩散生成时的图像和目标文本的多模态CLIP损失尽可能小。</p><p>CLIP Loss</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="script">L</mi><mrow><mi>d</mi><mi>i</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>g</mi><mi>e</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi></mrow></msub><mo separator="true">;</mo><msub><mi>x</mi><mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mo fence="true">⟨</mo><mi mathvariant="normal">Δ</mi><mi>I</mi><mo separator="true">,</mo><mi mathvariant="normal">Δ</mi><mi>T</mi><mo fence="true">⟩</mo></mrow><mrow><mrow><mo fence="true">∥</mo><mi mathvariant="normal">Δ</mi><mi>I</mi><mo fence="true">∥</mo></mrow><mrow><mo fence="true">∥</mo><mi mathvariant="normal">Δ</mi><mi>T</mi><mo fence="true">∥</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathcal{L} _{direction}(x_{gen},y_{tar};x_{ref},y_{ref}):=1-\frac{\left \langle \Delta I,\Delta T \right \rangle }{\left \| \Delta I \right \| \left \| \Delta T \right \| } </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">rec</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose delimcenter" style="top:0em;">∥</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="minner"><span class="mopen delimcenter" style="top:0em;">⟨</span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose delimcenter" style="top:0em;">⟩</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>I</mi></mrow><annotation encoding="application/x-tex">\Delta I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span></span></span></span> 是图像编码器对生成图和原图的编码向量差，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">\Delta T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span>是文本编码器对目标文本和原文本的编码向量的差。</p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230603133758788.png" alt="image-20230603133758788" style="zoom:67%;"><h1 id="vq-vae" tabindex="-1"><a class="header-anchor" href="#vq-vae"><span>VQ-VAE</span></a></h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/496148378" target="_blank" rel="noopener noreferrer">VQVAE原理解读 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/463043201" target="_blank" rel="noopener noreferrer">生成模型之VQ-VAE - 知乎 (zhihu.com)</a></p><p><a href="https://kexue.fm/archives/5253" target="_blank" rel="noopener noreferrer">变分自编码器（一）：原来是这么一回事 - 科学空间|Scientific Spaces (kexue.fm)</a></p><p><a href="https://kexue.fm/archives/5343" target="_blank" rel="noopener noreferrer">变分自编码器（二）：从贝叶斯观点出发 - 科学空间|Scientific Spaces (kexue.fm)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230603141158999.png" alt="image-20230603141158999"></p><h1 id="vqgan" tabindex="-1"><a class="header-anchor" href="#vqgan"><span>VQGAN</span></a></h1><blockquote><p>Taming Transformers for High-Resolution Image Synthesis</p><p><a href="https://paperswithcode.com/conference/cvpr-2021-1" target="_blank" rel="noopener noreferrer">CVPR 2021</a></p></blockquote><p>驯化transformer来生成高解析度图像</p><h2 id="问题" tabindex="-1"><a class="header-anchor" href="#问题"><span>问题</span></a></h2><p>图像的序列长度远比自然语言高。自然语言模型往往将生成序列的长度控制在1024或512内，但如果将自然语言模型里的transformer用来自回归式逐位生成像素的话，1024的长度只能生成32*32大小的图像。并且，attention的计算复杂度是随着序列长度的增长以平方级增长的，这样就限制了能生成图像的大小。虽然相比于CNN，transformer并不对输入进行任何先验的假设（例如平移不变性，局部性等）并且因此能够很好地拟合输入间复杂的关系，但这种普适泛化性也意味着你需要更充足的训练和更广泛的搜索范围。</p><p>**有没有办法既兼具CNN的先验偏置，又兼具transformer建模序列的泛化性？**这篇文章主要在探讨如何解决这个问题。作者提出了以下洞见：<strong>CNN的归纳偏置可以很好地概括图像的底层结构特性（例如它的局部性），但这种偏置对于语义层面（即图像的整体理解，全局把握）的建模用处不大。</strong></p><h2 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法</span></a></h2><p>提出了一种CNN+GAN+Transformer的结构来生成高精度图像。</p><ul><li>第一个步骤先训练一个VQVAE模型，其中CNN负责作为编码器，将图像编码成一个个具体且感知丰富（由感知损失Perceptual loss和GAN共同完成）的离散编码向量，再由解码器（也是CNN架构）还原原图像。</li><li>而在得到了编解码器后采样生成的第二个步骤时，训练一个transformer来学习第一步里的离散编码向量序列间的关系。</li></ul><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230603140606220.png" alt="image-20230603140606220"></p><p><strong>VQGAN或者DALL-E都使用了Transformer架构将潜在空间里的离散索引序列的建模问题转化为了一维的序列生成问题。</strong> ？？？？</p><h1 id="vqgan-clip" tabindex="-1"><a class="header-anchor" href="#vqgan-clip"><span>VQGAN-CLIP</span></a></h1><blockquote><p><a href="https://paperswithcode.com/paper/vqgan-clip-open-domain-image-generation-and" target="_blank" rel="noopener noreferrer">VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance | Papers With Code</a></p></blockquote><p>用一个多模态的编码器来计算文和图的编码向量的余弦相似度，并将该相似度以损失的形式传递给图像生成器，不断迭代直到收敛。而这种流程对于用文本引导从零生成和以图生图的区别仅在于输入是随机噪声还是给定图像。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230603145014579.png" alt="image-20230603145014579"></p><h1 id="stable-diffusion" tabindex="-1"><a class="header-anchor" href="#stable-diffusion"><span>Stable-Diffusion</span></a></h1><blockquote><p><a href="https://github.com/CompVis/stable-diffusion" target="_blank" rel="noopener noreferrer">CompVis/stable-diffusion: A latent text-to-image diffusion model (github.com)</a></p><p>CVPR 2022</p></blockquote><p>Disco-Diffusion：diffusion + clip，在全图像素上进行扩散，训练一个这样的模型需要数百个V100卡满载天数，而且下游推理同样费时费力。</p><p><a class="route-link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/l60xc115/">StableDiffusion</a></p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><ul><li><p>最本质来说，SD相当于VQGAN里的Transformer被替换成了diffusion model。</p></li><li><p>论文的另一个核心贡献是探索了使用cross-attention做多模态的条件扩散生成。</p></li></ul></div><!----><!----><!----></div></main><footer class="vp-doc-footer" data-v-a703f9d3 data-v-fda6bbae><!--[--><!--]--><!----><div class="contributors" aria-label="Contributors" data-v-fda6bbae><span class="contributors-label" data-v-fda6bbae>贡献者: </span><span class="contributors-info" data-v-fda6bbae><!--[--><!--[--><span class="contributor" data-v-fda6bbae>weiwen</span><!----><!--]--><!--]--></span></div><nav class="prev-next" data-v-fda6bbae><div class="pager" data-v-fda6bbae><a class="vp-link no-icon link pager-link prev" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/2gu6h6qk/" data-v-fda6bbae data-v-442a52aa><!--[--><span class="desc" data-v-fda6bbae>上一页</span><span class="title" data-v-fda6bbae>扩散模型与受控图像生成</span><!--]--><!----></a></div><div class="pager" data-v-fda6bbae><a class="vp-link no-icon link pager-link next" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/l60xc115/" data-v-fda6bbae data-v-442a52aa><!--[--><span class="desc" data-v-fda6bbae>下一页</span><span class="title" data-v-fda6bbae>StableDiffusion</span><!--]--><!----></a></div></nav></footer><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-d90a7a26 data-v-bcf8d9a6><span class="percent" data-allow-mismatch data-v-bcf8d9a6>0%</span><span class="show icon vpi-back-to-top" data-v-bcf8d9a6></span><svg aria-hidden="true" data-v-bcf8d9a6><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-bcf8d9a6></circle></svg></button><footer class="vp-footer has-sidebar" vp-footer data-v-d90a7a26 data-v-400675cf><!--[--><div class="container" data-v-400675cf><!----><p class="copyright" data-v-400675cf>wenwei@2025</p></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/archive2025/assets/app-IQA0dJD3.js" defer></script></body></html>