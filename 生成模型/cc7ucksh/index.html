<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.21" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.143" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><link rel="icon" type="image/png" href="https://theme-plume.vuejs.press/favicon-32x32.png"><title>DiffSeg | aurora</title><meta name="description" content=""><link rel="preload" href="/archive2025/assets/style-Nqw6QQQ5.css" as="style"><link rel="stylesheet" href="/archive2025/assets/style-Nqw6QQQ5.css"><link rel="modulepreload" href="/archive2025/assets/app-IQA0dJD3.js"><link rel="modulepreload" href="/archive2025/assets/index.html-CeoMv55w.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-d90a7a26><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d5a8d0bc></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-d5a8d0bc> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-d90a7a26 data-v-e98a6132><div class="vp-navbar" vp-navbar data-v-e98a6132 data-v-2c31ea5e><div class="wrapper" data-v-2c31ea5e><div class="container" data-v-2c31ea5e><div class="title" data-v-2c31ea5e><div class="vp-navbar-title" data-v-2c31ea5e data-v-1a4f50af><a class="vp-link no-icon link title" href="/archive2025/" data-v-1a4f50af data-v-442a52aa><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="https://theme-plume.vuejs.press/plume.png" alt data-v-eda4b9bd><!--]--><!--[--><img class="vp-image light logo" style="" src="https://theme-plume.vuejs.press/plume.png" alt data-v-eda4b9bd><!--]--><!--]--><!--]--><span data-v-1a4f50af>aurora</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-2c31ea5e><div class="content-body" data-v-2c31ea5e><!--[--><!--]--><div class="vp-navbar-search search" data-v-2c31ea5e><div class="search-wrapper" data-v-97535d1e><!----><div id="local-search" data-v-97535d1e><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-97535d1e><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-2c31ea5e data-v-d43c1732><span id="main-nav-aria-label" class="visually-hidden" data-v-d43c1732>Main Navigation</span><!--[--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>首页</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>博客</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/tags/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>标签</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/archives/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>归档</span><!----><!--]--><!----></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-d43c1732 data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-86530b6c><span class="text" data-v-86530b6c><!----><!----><span data-v-86530b6c>笔记</span><!----><span class="vpi-chevron-down text-icon" data-v-86530b6c></span></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><div class="items" data-v-709dc2b1><!--[--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/demo/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 示例 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/SLAM/zc7d97ub/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> SLAM <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-group" data-v-709dc2b1 data-v-c497e9e3><p class="title" data-v-c497e9e3><!----><span data-v-c497e9e3>AI</span></p><!--[--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 生成模型 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E6%84%9F%E7%9F%A5/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 感知 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E6%95%B0%E6%8D%AE%E9%9B%86/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 数据集 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E5%88%86%E5%B1%82%E8%AF%AD%E4%B9%89/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 分层语义 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/VLN/" data-v-1ff1855f data-v-442a52aa><!--[--><!---->  VLN <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-2c31ea5e data-v-a295abf6><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-a295abf6 data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-2c31ea5e data-v-ad52545c data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="/" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-2c31ea5e data-v-652282fd data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-86530b6c><span class="vpi-more-horizontal icon" data-v-86530b6c></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><!----><!--[--><!--[--><!----><div class="group" data-v-652282fd><div class="item appearance" data-v-652282fd><p class="label" data-v-652282fd>外观</p><div class="appearance-action" data-v-652282fd><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-652282fd data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-652282fd><div class="item social-links" data-v-652282fd><div class="vp-social-links social-links-list" data-v-652282fd data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="/" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-2c31ea5e data-v-2b50024d><span class="container" data-v-2b50024d><span class="top" data-v-2b50024d></span><span class="middle" data-v-2b50024d></span><span class="bottom" data-v-2b50024d></span></span></button></div></div></div></div><div class="divider" data-v-2c31ea5e><div class="divider-line" data-v-2c31ea5e></div></div></div><!----></header><!----><!----><!--[--><div id="VPContent" vp-content class="vp-content" data-v-d90a7a26 data-v-b2beaca7><div class="vp-doc-container has-aside" data-v-b2beaca7 data-v-a703f9d3><!--[--><!--]--><div class="container" data-v-a703f9d3><div class="aside" vp-outline data-v-a703f9d3><div class="aside-curtain" data-v-a703f9d3></div><div class="aside-container" data-v-a703f9d3><div class="aside-content" data-v-a703f9d3><div class="vp-doc-aside" data-v-a703f9d3 data-v-5976474c><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-5976474c data-v-aa56eba0><div class="content" data-v-aa56eba0><div class="outline-marker" data-v-aa56eba0></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-aa56eba0><span data-v-aa56eba0>此页内容</span><span class="vpi-print icon" data-v-aa56eba0></span></div><ul class="root" data-v-aa56eba0 data-v-3e6b023c><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-5976474c></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-a703f9d3><div class="content-container" data-v-a703f9d3><!--[--><!--]--><main class="main" data-v-a703f9d3><!----><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-27be53cb>DiffSeg <!----></h1><div class="vp-doc-meta" data-v-27be53cb><!--[--><!--]--><p class="reading-time" data-v-27be53cb><span class="vpi-books icon" data-v-27be53cb></span><span data-v-27be53cb>约 1757 字</span><span data-v-27be53cb>大约 6 分钟</span></p><!----><!--[--><!--]--><p class="create-time" data-v-27be53cb><span class="vpi-clock icon" data-v-27be53cb></span><span data-v-27be53cb>1984-01-24</span></p></div><!--]--><!--[--><!--]--><div class="_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B_cc7ucksh_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-a703f9d3><!--[--><!--]--><div data-v-a703f9d3><p>Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909113225632.png" alt="image-20230909113225632"></p><p>Aug.23.2023</p><h2 id="相关工作" tabindex="-1"><a class="header-anchor" href="#相关工作"><span>相关工作</span></a></h2><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909114846240.png" alt="image-20230909114846240" style="zoom:67%;"><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909114944332.png" alt="image-20230909114944332" style="zoom:67%;"><p>Transformer Decoder</p><blockquote><p>https://github.com/CompVis/latent-diffusion/blob/a506df5756472e2ebaf9078affdde2c4f1502cd4/ldm/modules/attention.py#L196C40-L196C40</p></blockquote><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">class</span><span style="--shiki-light:#2E8F82;--shiki-dark:#5DA994;"> BasicTransformerBlock</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">Module</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> n_heads</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> d_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dropout</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> context_dim</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">=</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">None</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> gated_ff</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">=</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">True</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> checkpoint</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">=</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">True</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        super</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">().</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">__init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">attn1 </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> CrossAttention</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">query_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> heads</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">n_heads</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> dim_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">d_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> dropout</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dropout</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # is a self-attention</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">ff </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> FeedForward</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> dropout</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dropout</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> glu</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">gated_ff</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">attn2 </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> CrossAttention</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">query_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> context_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">context_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">                                    heads</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">n_heads</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> dim_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">d_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> dropout</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dropout</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # is self-attn if context is none</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">norm1 </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">LayerNorm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">norm2 </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">LayerNorm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">norm3 </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">LayerNorm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">checkpoint </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> checkpoint</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> forward</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> context</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">=</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">None</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        return</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> checkpoint</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">_forward</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> (</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> context</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">),</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">parameters</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(),</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">checkpoint</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> _forward</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> context</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">=</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">None</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        x </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">attn1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">norm1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">))</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> +</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> x</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        x </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">attn2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">norm2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">),</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> context</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">context</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> +</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> x</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        x </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">ff</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">norm3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">))</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> +</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> x</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        return</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> x</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法</span></a></h2><h3 id="理论依据" tabindex="-1"><a class="header-anchor" href="#理论依据"><span>理论依据</span></a></h3><h4 id="intra-attention-similarity" tabindex="-1"><a class="header-anchor" href="#intra-attention-similarity"><span>Intra-Attention Similarity</span></a></h4><p>一张 attn map 中，同一个 object group 附近相似。</p><h4 id="inter-attention-similarity" tabindex="-1"><a class="header-anchor" href="#inter-attention-similarity"><span>Inter-Attention Similarity</span></a></h4><p>同一个 object group 具有相似 attn map 。</p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909113340573.png" alt="image-20230909113340573" style="zoom:67%;"><h3 id="attention-map" tabindex="-1"><a class="header-anchor" href="#attention-map"><span>attention map</span></a></h3><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>w</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>∈</mo><mo stretchy="false">{</mo><mn>8</mn><mo>×</mo><mn>8</mn><mo separator="true">,</mo><mn>16</mn><mo>×</mo><mn>16</mn><mo separator="true">,</mo><mn>32</mn><mo>×</mo><mn>32</mn><mo separator="true">,</mo><mn>64</mn><mo>×</mo><mn>64</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">(h_k,w_k)\in\{8\times8,16\times16,32\times32,64\times64\} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">16</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">32</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">32</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">64</span><span class="mclose">}</span></span></span></span></span></p><p>总共16个注意力层</p><ul><li>使用无条件的 latent</li><li>仅运行一次SD <ul><li>t = 300，近似无噪音输入</li></ul></li></ul><div class="hint-container tip"><p class="hint-container-title">如何处理多头注意力</p><p>由于多头注意力机制，存在第五个维度。</p><p>每个注意力层都有 8 个多头输出。我们沿着多头轴平均注意力张量以减少到 4 个维度，因为它们在这个维度上非常相似。</p><p>实验中，多头之间的差异远小于不同map之间kl散度的阈值，即使不平均在后面也会被合并。</p></div><h3 id="attention-aggregation" tabindex="-1"><a class="header-anchor" href="#attention-aggregation"><span>Attention Aggregation</span></a></h3><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909122303224.png" alt="image-20230909122303224"></p><ul><li><p>对齐张量</p><ul><li>不同大小的map使用双线性插值扩大 <ul><li>归一化，保持和为1</li></ul></li><li>不同数量的map进行复制</li></ul></li><li><p>相加，聚合成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn><mo>×</mo><mn>64</mn><mo>×</mo><mn>64</mn><mo>×</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">64\times64\times64\times64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span></span></span></span></p><ul><li>聚合权重，正比于map分辨率</li></ul></li></ul><h3 id="iterative-attention-merging" tabindex="-1"><a class="header-anchor" href="#iterative-attention-merging"><span>Iterative Attention Merging</span></a></h3><ul><li>采样 64 个点（anchors）</li></ul><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909122641936.png" alt="image-20230909122641936" style="zoom:40%;"><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909122143496.png" alt="image-20230909122143496" style="zoom:40%;"></p><blockquote><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">L_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，sample grid，从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">A_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>中采样</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">A_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>, attn maps, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn><mo>×</mo><mn>64</mn><mo>×</mo><mn>64</mn><mo>×</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">64\times64\times64\times64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span></span></span></span></p><p>N, iterations,</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span></span></span></span>, threshold</p></blockquote><ul><li><p>第一次迭代</p><ul><li>根据相似程度，将不同的maps聚类到不同的anchors上 <ul><li>KL散度，<img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909130713818.png" alt="image-20230909130713818" style="zoom:50%;"></li><li>KL散度具有方向，取正反两个方向相加</li></ul></li><li>对每个聚类的anchors中的maps进行平均，得到这个anchor的attn map</li></ul></li><li><p>后续</p><ul><li>与第一次迭代类似，不同的是对anchor进行聚类和平均，减少anchor数量 <ul><li>类似 mean shift</li></ul></li></ul></li><li><p>双线性插值</p></li><li><p>非极大抑制</p></li></ul><div class="hint-container tip"><p class="hint-container-title">提示</p><p>KL 散度不是距离指标，没有对称性</p><p>用其他方式衡量相似的？</p><p>Wasserstein Distance</p><p><a href="https://zhuanlan.zhihu.com/p/58506295" target="_blank" rel="noopener noreferrer">【数学】Wasserstein Distance - 知乎 (zhihu.com)</a></p><p>Covariance</p></div><h2 id="结果" tabindex="-1"><a class="header-anchor" href="#结果"><span>结果</span></a></h2><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909113631210.png" alt="image-20230909113631210"></p><h2 id="类似工作" tabindex="-1"><a class="header-anchor" href="#类似工作"><span>类似工作</span></a></h2><h3 id="affinitynet" tabindex="-1"><a class="header-anchor" href="#affinitynet"><span>AffinityNet</span></a></h3><blockquote><p><a href="https://github.com/jiwoon-ahn/psa" target="_blank" rel="noopener noreferrer">jiwoon-ahn/psa: Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation, CVPR 2018 (github.com)</a></p></blockquote><p><a href="https://blog.csdn.net/qq_36686169/article/details/106234954" target="_blank" rel="noopener noreferrer">论文笔记]AffinityNet_动如脱兔((≡ຶ̑ꀬ≡ຶ̑))静如脱兔的博客-CSDN博客</a></p><p>输入：原始图像</p><p>输出：图像中相邻坐标像素对的语义相似度</p><p>仅通过图像级标签预测像素级别的语义相似度</p><ul><li><p>根据给定的图像和生成的CAM，首先建立一个邻域图，其中每个像素都在一定半径内与其邻域相连，通过AffinityNet估计图中连通像素对的语义相似度。</p></li><li><p>针对每一个类别，CAM中稀疏的激活区域通过随机游走策略传播到周围语义相同的区域，并对传播到其他语义类别的区域进行处罚。这种语义扩散对CAMs区域起到了很大的修正作用。</p></li><li><p>本文利用这个过程来进行训练，通过获取的每个像素位置与CAM对应类标签的关联性合成像素级的分割标签。生成的分割标签用于训练分割模型。</p></li></ul><blockquote><p>类似文章，把cnn的feature 换成了 transform 的feature</p><p><a href="https://github.com/rulixiang/afa" target="_blank" rel="noopener noreferrer">CVPR 2022] Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers (github.com)</a></p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/afa.png" alt="AFA flowchart"></p></blockquote><h3 id="diffumasks" tabindex="-1"><a class="header-anchor" href="#diffumasks"><span>DiffuMasks</span></a></h3><p><a href="https://github.com/weijiawu/DiffuMask" target="_blank" rel="noopener noreferrer">weijiawu/DiffuMask: DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models (github.com)</a></p><p>ICCV 2023</p><p>生成图片的同时生成mask，跨模态，文本监督</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909142049679.png" alt="image-20230909142049679"></p><h3 id="reco" tabindex="-1"><a class="header-anchor" href="#reco"><span>ReCo</span></a></h3><p>Reco: Retrieve and co-segment for zero-shot transfer.</p><p><a href="https://github.com/NoelShin/reco" target="_blank" rel="noopener noreferrer">NeurIPS&#39;22 ReCo: Retrieve and Co-segment for Zero-shot Transfer (github.com)</a></p><p><a href="https://www.robots.ox.ac.uk/~vgg/research/reco/" target="_blank" rel="noopener noreferrer">ReCo (ox.ac.uk)</a></p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909142643824.png" alt="image-20230909142643824"></p><p>不懂</p><p>ReCo 的输入是未标记图像的集合以及要分割的概念的文本描述列表。通过将图像检索和图像集合中的联合分割相结合，ReCo 可以动态地为给定的概念构建分割器。在推理过程中，应用该分割器时无需对感兴趣的目标分布中的图像进行微调，从而支持零样本传输。</p><div class="hint-container tip"><p class="hint-container-title">Dense Clip</p><p>将 clip 的 text - image 工作进一步拓展到 text - pixel ，以实现分割</p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/v2-f62162a07e079101b97e107eb8c489e1_1440w.webp" alt="img" style="zoom:67%;"><p><a href="https://zhuanlan.zhihu.com/p/493775034" target="_blank" rel="noopener noreferrer">【CLIP系列Paper解读】DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting - 知乎 (zhihu.com)</a></p></div><h3 id="network-free" tabindex="-1"><a class="header-anchor" href="#network-free"><span>Network-free</span></a></h3><p>cvpr‘2023</p><p><a href="https://paperswithcode.com/paper/network-free-unsupervised-semantic" target="_blank" rel="noopener noreferrer">Network-Free, Unsupervised Semantic Segmentation With Synthetic Images | Papers With Code</a></p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909133606320.png" alt="image-20230909133606320"></p><div class="hint-container tip"><p class="hint-container-title">核心idea</p><p>当使用 GAN 中的风格混合方法生成图像的合成变体时，属于同一语义段的一组像素的相关性不会改变。</p></div><h3 id="toco" tabindex="-1"><a class="header-anchor" href="#toco"><span>ToCo</span></a></h3><p><a href="https://github.com/rulixiang/toco" target="_blank" rel="noopener noreferrer">CVPR 2023 Token Contrast for Weakly-Supervised Semantic Segmentation (github.com)</a></p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/toco.png" alt="AFA flowchart"></p><blockquote><p>CAM: Class Activation Map, 分类网络预测图像时的激活区域。</p><p>EMA: 指数移动平均（Exponential Moving Average）也叫权重移动平均（Weighted Moving Average），是一种给予近期数据更高权重的平均方法。</p><p><a href="https://zhuanlan.zhihu.com/p/68748778" target="_blank" rel="noopener noreferrer">【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现 - 知乎 (zhihu.com)</a></p></blockquote><ul><li>vit存在的过度平滑问题,feature map</li></ul><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/7d2028f71bcd1b27eb49026e7f28d3ff.png" alt="7d2028f71bcd1b27eb49026e7f28d3ff.png" style="zoom:67%;"><h4 id="ptc-patch-token-contrast" tabindex="-1"><a class="header-anchor" href="#ptc-patch-token-contrast"><span>PTC：Patch Token Contrast</span></a></h4><p>利用中间层（第十个encoder)的输出外接辅助分类层得到Mm，主要是因为中间层仍然可以保持语义多样性，由它生成的token relations Y作为global patch tokens F的监督，可以解决vit存在的过度平滑问题。</p><h4 id="ctc-classtokencontrast" tabindex="-1"><a class="header-anchor" href="#ctc-classtokencontrast"><span>CTC：ClassTokenContrast</span></a></h4><p>PTC解决了vit存在的过度平滑问题，生成效果不错的辅助cam。然而，仍然存在一些识别能力较弱的难以区分的对象区域。</p><p>受ViT中class token 可以聚合高级语义的特性，设计了(class Token Contrast, CTC)模块，提高伪标签质量。</p><ul><li><p>从辅助CAM Mm指定的不确定区域和背景区域对原始图像进行随机裁剪得到局部图像，将其分配为正样本(来自不确定区域)或负样本(来自背景区域)。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230909140248549.png" alt="image-20230909140248549"></p></li><li><p>通过最小化全局class token P和正样本之间的差异（在不确定区域中激活出更多的前景区域），最大化全局class token P和负样本之间的差异（加大前景和背景的差异），</p></li><li><p>整个ctc loss 通过最小化全局class token 和局部class token 来促进局部非显著区域与全局对象之间表示的一致性，从而进一步强制从CAM中激活更多的对象区域。</p></li></ul></div><!----><!----><!----></div></main><footer class="vp-doc-footer" data-v-a703f9d3 data-v-fda6bbae><!--[--><!--]--><!----><div class="contributors" aria-label="Contributors" data-v-fda6bbae><span class="contributors-label" data-v-fda6bbae>贡献者: </span><span class="contributors-info" data-v-fda6bbae><!--[--><!--[--><span class="contributor" data-v-fda6bbae>weiwen</span><!----><!--]--><!--]--></span></div><!----></footer><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-d90a7a26 data-v-bcf8d9a6><span class="percent" data-allow-mismatch data-v-bcf8d9a6>0%</span><span class="show icon vpi-back-to-top" data-v-bcf8d9a6></span><svg aria-hidden="true" data-v-bcf8d9a6><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-bcf8d9a6></circle></svg></button><footer class="vp-footer" vp-footer data-v-d90a7a26 data-v-400675cf><!--[--><div class="container" data-v-400675cf><!----><p class="copyright" data-v-400675cf>wenwei@2025</p></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/archive2025/assets/app-IQA0dJD3.js" defer></script></body></html>