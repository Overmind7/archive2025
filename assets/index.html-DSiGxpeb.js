import{_ as e,c as i,b as s,o as n}from"./app-IQA0dJD3.js";const t={};function o(r,a){return n(),i("div",null,a[0]||(a[0]=[s('<h2 id="unconditional-ddpm" tabindex="-1"><a class="header-anchor" href="#unconditional-ddpm"><span>unconditional DDPM</span></a></h2><p>根据<strong>郎之万动力方程</strong>的推导，最终的生成表达式中依赖神经网络对噪声的预测可以生成图像，但是这种生成是没有任何约束的，也就是说给定纯高斯噪声，我们就能生成图片。好处是我们的输入不受任何控制，只要是高斯噪声就可以，坏处是我们无法监督这一过程，最终生成的结果不受控制。</p><h2 id="guided-diffusion-diffusion-models-beat-gans-on-image-synthesis" tabindex="-1"><a class="header-anchor" href="#guided-diffusion-diffusion-models-beat-gans-on-image-synthesis"><span>Guided Diffusion/Diffusion Models Beat GANs on Image Synthesis</span></a></h2><blockquote><p>https://proceedings.neurips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf</p><p>第一次，classifier guidance</p></blockquote><h2 id="ilvr-conditioning-method-for-denoising-diffusion-probabilistic-models" tabindex="-1"><a class="header-anchor" href="#ilvr-conditioning-method-for-denoising-diffusion-probabilistic-models"><span>ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models</span></a></h2><blockquote><p>sample全程加入参考图片</p><p><a href="https://zhuanlan.zhihu.com/p/401225344" target="_blank" rel="noopener noreferrer">ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models - 知乎 (zhihu.com)</a></p><p><a href="https://arxiv.org/pdf/2108.02938v2.pdf" target="_blank" rel="noopener noreferrer">2108.02938v2.pdf (arxiv.org)</a></p></blockquote><p>这篇论文是直接基于DDPM的工作上展开的受控图像生成。其主要针对的痛点是DDPM的随机性太高无法确定性的生成，导致我们很难控制模型生成带有我们想要的语义信息的图片。</p><p>其核心思想非常地简洁但精巧：</p><ul><li>我们的前向和后向是一个等长的过程。其中前向时原数据的信息逐渐丢失（先丢失高频信息再丢失低频信息）而后向时信息逐渐从纯噪声中补全（先补全低频信息再到高频信息）。</li><li>如果我们记录下前向过程里每一步的噪声图像，将其与后向过程中的噪声图像混合，我们就可以影响后向过程的生成结果（考虑极端情况完全替换后向过程的噪声图像的话则一定可以轻易地回到原图）。</li><li>而<strong>我们通过影响混合时注入的前向信息的多少，或者后向时注入信息的时间步的多少，可以控制所生成的图与原图的相似程度</strong>。具体来说，其算法如下：其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">\\phi_N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是一个<strong>低通滤波器加上一系列降维再升维保持图像维度不变</strong>的过程。</li></ul><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230726231846203.png" alt="image-20230726231846203"></p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230726231902871.png" alt="image-20230726231902871"></p><h3 id="创新点" tabindex="-1"><a class="header-anchor" href="#创新点"><span>创新点</span></a></h3><p>DDPM模型已经在无条件生成方面取得了显著的成就，然而，由于其生成过程的随机性，对该模型做可控生成十分具有挑战性。</p><ul><li>本文提出了一个Iterative Latent Variable Refinement(ILVR)来指导DDPM在给定reference image的条件下能够生成高质量的图片样本。</li><li>同时，这样控制的方法可以让一个DDPM模型在无需额外模型或学习过程参与的情况下适用于multi-domain image translation，editing with scribbles等应用任务。</li></ul><h2 id="diffusion-probabilistic-models-for-3d-point-cloud-generation" tabindex="-1"><a class="header-anchor" href="#diffusion-probabilistic-models-for-3d-point-cloud-generation"><span>Diffusion Probabilistic Models for 3D Point Cloud Generation</span></a></h2><blockquote><p><a href="https://blog.csdn.net/qq_41178930/article/details/121843653?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-121843653-blog-122078131.235%5Ev36%5Epc_relevant_default_base3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-121843653-blog-122078131.235%5Ev36%5Epc_relevant_default_base3&amp;utm_relevant_index=3" target="_blank" rel="noopener noreferrer">点云生成：Diffusion Probabilistic Models for 3D Point Cloud Generation_</a></p></blockquote><p>使用学习到的潜变量 Z 控制点云形状</p><blockquote><p>由于马尔可夫链的目的是对点分布进行建模，因此仅靠马尔可夫链无法生成各种形状的点云。为此，我们引入了一个潜在的形状作为过渡内核的条件。在生成设置中，潜在形状遵循先验分布，我们通过归一化流 [5、6] 对其进行参数化，以获得强大的模型表现力。</p><p>在自动编码的设置中，潜在的形状是端到端学习的。最后，我们将训练目标制定为最大化以潜在形状为条件的点云似然的变分下界，进一步将其制定为封闭形式的易处理表达式。我们将我们的模型应用于点云生成、自动编码和无监督表示学习，结果表明我们的模型在点云生成和自动编码方面取得了有竞争力的性能，并且在无监督表示学习方面取得了可比的结果。</p></blockquote><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230524093946010.png" alt="image-20230524093946010"></p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20230524094008400.png" alt="image-20230524094008400"></p>',20)]))}const l=e(t,[["render",o]]),d=JSON.parse('{"path":"/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/fmdki2js/","title":"条件扩散","lang":"zh-CN","frontmatter":{"title":"条件扩散","createTime":"1984/01/24 16:00:00","permalink":"/生成模型/fmdki2js/"},"readingTime":{"minutes":3.39,"words":1017},"git":{"updatedTime":1750820984000,"contributors":[{"name":"weiwen","username":"","email":"a1036359215@163.com","commits":1,"avatar":"https://gravatar.com/avatar/cd8e1d2cae5eb43df4bdc241dd3c0611439067bff22550061317525e3b170bab?d=retro"}]},"filePathRelative":"notes/生成模型/diffusion/条件扩散.md","headers":[],"categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"bdd74e","sort":10006,"name":"生成模型"},{"id":"9bc14f","sort":10010,"name":"diffusion"}]}');export{l as comp,d as data};
