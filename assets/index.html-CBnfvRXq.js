import{_ as a,c as d,b as s,a as n,d as r,w as o,r as l,o as p,e}from"./app-IQA0dJD3.js";const m={},u={id:"effocc",tabindex:"-1"},c={class:"header-anchor",href:"#effocc"},g={id:"daocc",tabindex:"-1"},f={class:"header-anchor",href:"#daocc"},h={id:"gaussianformer3d",tabindex:"-1"},b={class:"header-anchor",href:"#gaussianformer3d"},F={id:"occfusion",tabindex:"-1"},O={class:"header-anchor",href:"#occfusion"},D={id:"occcylindrical",tabindex:"-1"},E={class:"header-anchor",href:"#occcylindrical"};function v(x,t){const i=l("RouteLink");return p(),d("div",null,[t[5]||(t[5]=s('<p>多模态融合 Occ 预测</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>L + C</p></div><table><thead><tr><th>项目</th><th>经典 LSS</th><th>LSSViewTransformer</th></tr></thead><tbody><tr><td>输出空间</td><td>BEV 2D plane</td><td>Sparse 3D voxel grid</td></tr><tr><td>Lift-to-3D</td><td>Yes</td><td>Yes</td></tr><tr><td>Splat-to-BEV</td><td>Yes</td><td>❌（而是聚合到3D voxel）</td></tr><tr><td>用途</td><td>BEV语义理解、检测等</td><td>Occupancy 预测</td></tr></tbody></table>',3)),n("h2",u,[n("a",c,[n("span",null,[r(i,{to:"/notes/%E6%84%9F%E7%9F%A5/fusionocc/EFFOcc.html"},{default:o(()=>t[0]||(t[0]=[e("EFFOcc")])),_:1})])])]),t[6]||(t[6]=n("p",null,"2025",-1)),t[7]||(t[7]=n("p",null,[n("a",{href:"https://arxiv.org/pdf/2406.07042",target:"_blank",rel:"noopener noreferrer"},"https://arxiv.org/pd f/ ")],-1)),t[8]||(t[8]=n("p",null,[n("a",{href:"https://github.com/synsin0/EFFOcc",target:"_blank",rel:"noopener noreferrer"},"https://github.com/synsin0/EFFOcc")],-1)),t[9]||(t[9]=n("p",null,[n("img",{src:"https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250609092646681.png",alt:"image-20250609092646681"})],-1)),n("h2",g,[n("a",f,[n("span",null,[r(i,{to:"/notes/%E6%84%9F%E7%9F%A5/fusionocc/DAOcc.html"},{default:o(()=>t[1]||(t[1]=[e("DAOcc")])),_:1})])])]),t[10]||(t[10]=n("p",null,[n("img",{src:"https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250624141703771.png",alt:"image-20250624141703771"})],-1)),n("h2",h,[n("a",b,[n("span",null,[r(i,{to:"/notes/%E6%84%9F%E7%9F%A5/fusionocc/GaussianFormer3D.html"},{default:o(()=>t[2]||(t[2]=[e("GaussianFormer3D")])),_:1})])])]),t[11]||(t[11]=n("p",null,"Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention",-1)),t[12]||(t[12]=n("p",null,"https://arxiv.org/pdf/2505.10685",-1)),t[13]||(t[13]=n("p",null,"https://lunarlab-gatech.github.io/GaussianFormer3D/#",-1)),t[14]||(t[14]=n("p",null,"代码暂未开源",-1)),t[15]||(t[15]=n("p",null,[n("img",{src:"https://lunarlab-gatech.github.io/GaussianFormer3D/static/images/gaussianformer3d_method.jpg",alt:"图2"})],-1)),n("h2",F,[n("a",O,[n("span",null,[r(i,{to:"/notes/%E6%84%9F%E7%9F%A5/fusionocc/OccFusion.html"},{default:o(()=>t[3]||(t[3]=[e("OCCFusion")])),_:1})])])]),t[16]||(t[16]=s('<p>Multi-Sensor Fusion Framework for 3D Semantic Occupancy Prediction</p><p>https://github.com/DanielMing123/OCCFusion</p><p>https://arxiv.org/pdf/2403.01644</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250611085306456.png" alt="image-20250611085306456"></p><p>将环视图像输入 2D 主干网络以提取多尺度特征。</p><p>对每个尺度进行视图变换，以获得该层级的全局 BEV 特征和局部 3D 特征体积。</p><p>同时，将激光雷达和环视雷达生成的 3D 点云输入 3D 主干网络，以分别生成多尺度的局部 3D 特征体积和全局 BEV 特征。</p><p>将每个层级的合并后的全局 BEV 特征和局部 3D 特征体积输入全局 - 局部注意力融合模块，以生成每个尺度的最终 3D 体积。</p><p>对每个层级的 3D 体积进行上采样，并执行跳跃连接，同时采用多尺度监督机制。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250611090521752.png" alt="image-20250611090521752"></p><table><thead><tr><th>方法</th><th>延迟（ms）（↓）</th><th>内存（GB）（↓）</th><th>参数</th></tr></thead><tbody><tr><td>SurroundOcc [7]</td><td>472</td><td>5.98</td><td>180.51M</td></tr><tr><td>InverseMatrixVT3D [14]</td><td>447</td><td>4.41</td><td>67.18M</td></tr><tr><td>OccFusion（C+R）</td><td>588</td><td>5.56</td><td>92.71M</td></tr><tr><td>OccFusion（C+L）</td><td>591</td><td>5.56</td><td>92.71M</td></tr><tr><td>OccFusion（C+L+R）</td><td>601</td><td>5.78</td><td>114.97M</td></tr></tbody></table><p>表 VI：不同方法的模型效率比较。<mark class="important">实验在单个 A10 上进行</mark>，使用六个多相机图像、激光雷达和雷达数据。对于输入图像分辨率，所有方法均采用 1600×900。↓：越低越好。</p>',12)),n("h2",D,[n("a",E,[n("span",null,[r(i,{to:"/notes/%E6%84%9F%E7%9F%A5/fusionocc/OccCylindrical.html"},{default:o(()=>t[4]||(t[4]=[e("OccCylindrical")])),_:1})])])]),t[17]||(t[17]=n("p",null,"Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction",-1)),t[18]||(t[18]=n("p",null,"https://www.arxiv.org/pdf/2505.03284",-1)),t[19]||(t[19]=n("p",null,"代码链接失效",-1)),t[20]||(t[20]=n("p",null,[n("img",{src:"https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250611093723117.png",alt:"image-20250611093723117"})],-1)),t[21]||(t[21]=n("p",null,"OccCylindrical的整体架构如下：首先，将环视图像通过二维主干网络进行初步处理，提取视觉特征。随后，利用DepthNet基于这些视觉特征生成深度分布特征，并使用激光雷达点云中的深度信息对深度分布特征进行监督。与此同时，将预定义的深度分布坐标（作为位置嵌入）与深度特征一起重新融合到视觉特征中，从而得到深度感知上下文特征。对深度分布特征和深度感知上下文特征进行外积操作，得到伪三维点云。TPV-Polar-Fusion模块以伪三维点云和激光雷达点云作为输入，进行特征级融合，并输出三个TPV-Polar平面。共享的编码器 - 解码器结构进一步细化TPV-Polar平面，并将细化后的TPV-Polar平面输出到预测头，用于三维语义占位预测。",-1)),t[22]||(t[22]=n("p",null,[n("img",{src:"https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250611093307874.png",alt:"image-20250611093307874"})],-1))])}const w=a(m,[["render",v]]),V=JSON.parse('{"path":"/%E6%84%9F%E7%9F%A5/fusion/09ay8hzz/","title":"概览","lang":"zh-CN","frontmatter":{"title":"概览","createTime":"2025/06/25 11:15:01","permalink":"/感知/fusion/09ay8hzz/"},"readingTime":{"minutes":2.35,"words":706},"git":{"updatedTime":1750822849000,"contributors":[{"name":"weiwen","username":"","email":"a1036359215@163.com","commits":2,"avatar":"https://gravatar.com/avatar/cd8e1d2cae5eb43df4bdc241dd3c0611439067bff22550061317525e3b170bab?d=retro"}]},"filePathRelative":"notes/感知/fusionocc/概览.md","headers":[],"categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"fcfb7c","sort":10007,"name":"感知"},{"id":"157e89","sort":10013,"name":"fusionocc"}]}');export{w as comp,V as data};
