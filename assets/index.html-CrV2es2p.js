import{_ as c,c as e,b as s,o as n}from"./app-IQA0dJD3.js";const p={};function r(h,a){return n(),e("div",null,a[0]||(a[0]=[s('<p>作者：</p><p>Zichen Yu¹，Changyong Shu²³，Jiajun Deng³，Kangjie Lu²，Zongdai Liu²，</p><p>Jiangyong Yu²，Dawei Yang²，Hui Li²，Yan Chen²</p><p>¹大连理工大学，²侯默AI，³阿德莱德大学</p><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>鉴于其在缓解3D物体检测中普遍存在的长尾缺陷和复杂形状缺失方面的能力，占据预测已成为自动驾驶系统中的关键组成部分。然而，三维体素级表示的处理不可避免地引入了巨大的内存和计算开销，阻碍了当前占据预测方法的部署。与模型更大、更复杂的趋势相反，我们认为理想的框架应该对多样化芯片部署友好，同时保持高精度。为此，我们提出了一种即插即用的范式，即FlashOCC，以在保持高精度的同时实现快速且内存高效的占据预测。具体而言，我们的FlashOCC基于当代体素级占据预测方法进行了两项改进。首先，特征保持在鸟瞰图（BEV）中，从而能够使用高效的2D卷积层进行特征提取。其次，引入了通道到高度变换，将BEV的输出逻辑提升到3D空间中。我们将FlashOCC应用于具有挑战性的Occ3D-nuScenes基准测试中的多种占据预测基线，并进行了广泛的实验以验证其有效性。结果证实了我们的即插即用范式在精度、运行时效率和内存成本方面优于先前的最先进方法，展示了其部署潜力。代码将公开提供。</p><h2 id="_1-引言" tabindex="-1"><a class="header-anchor" href="#_1-引言"><span>1 引言</span></a></h2><p>在环绕视角感知中，3D物体检测在自动驾驶系统中扮演着至关重要的角色。特别是，基于图像的3D感知因其比依赖激光雷达的解决方案更低的成本及其有前景的性能，受到了学术界和工业界越来越多的关注[9, 11, 12, 16, 27, 35]。然而，3D物体检测任务仅限于在预定义类别内生成边界框，这带来了两大挑战。首先，它面临长尾缺陷问题，即在现实场景中出现的未标记类别超出了现有的预定义类别。其次，它面临复杂形状缺失问题，因为现有检测方法无法充分捕捉多样化物体的复杂几何形状。</p><p>最近，占据预测这一新兴任务通过预测3D空间中每个体素的语义类别，解决了上述挑战[10, 14, 22, 23]。这种方法允许识别不符合预定义类别的物体，并将其标记为一般物体。通过在体素级特征上操作，这些方法能够更详细地表示场景，捕捉复杂形状并解决物体检测中的长尾缺陷问题。</p><p>占据预测的核心在于有效构建3D场景。传统方法采用体素化技术，将3D空间划分为体素，并为每个体素分配一个向量以表示其占据状态。尽管这些方法准确，但使用三维体素级表示引入了复杂的计算，包括3D（可变形）卷积、变换算子等[21, 22, 24, 28, 31, 36]。这些在芯片部署和计算能力需求方面带来了重大挑战。为了缓解这些挑战，稀疏占据表示[33]和三视角表示[10]被研究以节省内存资源。然而，这种方法并未从根本上解决部署和计算的挑战。</p><p>受子像素卷积技术[26]的启发，其中图像上采样通过通道重排实现，从而实现了通道到空间的特征变换。相应地，在我们的工作中，我们旨在高效实现通道到高度的特征变换。鉴于BEV感知任务的进展，其中BEV表示中的每个像素包含沿高度维度对应柱中所有物体的信息，我们直观地利用通道到高度变换将扁平化的BEV特征重塑为三维体素级占据逻辑。因此，我们专注于以通用和即插即用的方式增强现有模型，而非开发新的模型架构，如图1(a)所示。具体而言，我们直接将当代方法中的3D卷积替换为2D卷积，并将从3D卷积输出中获得的占据逻辑替换为通过2D卷积获得的BEV级特征的通道到高度变换。这些模型不仅在精度和时间消耗之间实现了最佳平衡，还展示了出色的部署兼容性。</p><h2 id="_2-相关工作" tabindex="-1"><a class="header-anchor" href="#_2-相关工作"><span>2 相关工作</span></a></h2><p><strong>体素级3D占据预测</strong></p><p>3D占据预测的最早起源可以追溯到占据网格地图（OGM）[30]，其旨在从图像中提取3D场景的详细结构信息，并促进下游规划和导航任务。现有研究根据监督类型可分为稀疏感知和密集感知。稀疏感知类别从激光雷达点云中获得直接监督，并在激光雷达数据集上进行评估[10]。同时，密集感知与语义场景补全（SSC）[2, 4]有相似之处。Voxformer[14]利用2.5D信息生成候选查询，然后通过插值获得所有体素特征。Occ3D[31]重新设计了一个由粗到细的体素编码器以构建占据表示。RenderOcc[22]通过2D到3D网络从环绕视角中提取3D体积特征，并使用Nerf监督预测每个体素的密度和标签。此外，还提出了几个带有密集占据标签的基准测试[28, 31]。上述方法通过向量描述每个体素对3D空间进行体素化[3, 14, 15, 22, 23, 24, 28]，因为具有细粒度3D结构的体素级表示天生非常适合3D语义占据预测。然而，基于体素的表示带来的计算复杂性和部署挑战促使我们寻求更高效的替代方案。</p><p><strong>基于BEV的3D场景感知</strong></p><p>基于BEV的方法使用一个向量表示BEV网格上整个柱的特征。与基于体素的方法相比，它在高度维度上减少了特征表示以提高计算效率，同时也避免了3D卷积的需求，使其更易于部署。在多样化的3D场景感知任务中已经展示了有前景的结果，例如3D车道检测[32]、深度估计[34]、3D物体检测[9, 12, 19]和3D物体跟踪[37]。尽管目前没有基于BEV级特征执行占据预测的方法，但BEV级特征可以隐式捕捉高度信息，这在不平坦路面或悬挂物体的场景中已得到验证。这些发现促使我们利用BEV级特征进行高效的占据预测。</p><p><strong>高效的子像素范式</strong></p><p>子像素卷积层最初在图像超分辨率[26]中提出，能够以极少的额外计算成本将低分辨率数据超分辨到高分辨率空间。同样的思想也被应用于BEV分割[17]，其中8×8网格大小的分割表示由一个分割查询描述，因此仅使用625个分割查询来预测最终的200×200 BEV分割结果。基于上述方法，我们提出了通道到高度变换作为占据预测的高效方法，其中占据逻辑通过通道到高度变换直接从扁平化的BEV级特征重塑而来。据我们所知，我们是首个将子像素范式应用于占据任务并完全避免计算密集型3D卷积的研究者。</p><h2 id="_3-框架" tabindex="-1"><a class="header-anchor" href="#_3-框架"><span>3 框架</span></a></h2><p>FlashOcc通过成功实现高精度的实时环绕视角3D占据预测，在该领域做出了开创性贡献。此外，由于避免了昂贵的体素级特征处理（如视图变换或3D（可变形）卷积算子），它在多样化的车载平台上展现出更强的部署灵活性。如图2所示，FlashOcc的输入数据为环绕视角图像，输出为密集占据预测结果。尽管我们的FlashOcc专注于以通用和即插即用的方式增强现有模型，但仍可分解为五个基本模块：</p><ol><li>2D图像编码器，负责从多相机图像中提取图像特征。</li><li>视图变换模块，促进从2D感知视图图像特征到3D BEV表示的映射。</li><li>BEV编码器，负责处理BEV特征信息。</li><li>占据预测模块，预测每个体素的分割标签。</li><li>可选的时间融合模块，旨在整合历史信息以提高性能。</li></ol><h3 id="_3-1-图像编码器" tabindex="-1"><a class="header-anchor" href="#_3-1-图像编码器"><span>3.1 图像编码器</span></a></h3><p>图像编码器从输入图像中提取感知视图中的高级特征。具体而言，它利用骨干网络提取多尺度语义特征，随后输入到颈部模块进行融合，从而充分利用具有多样化粒度的语义信息。经典的ResNet[8]和强大的SwinTransformer[18]通常被选为骨干网络。ResNet的多残差块设计使其能够优雅地获取具有丰富和多粒度语义信息的特征表示。Swin Transformer引入了分层结构，将输入图像划分为小块并逐步处理。通过利用移位窗口机制，SwinTransformer在保持多样化基准测试中竞争力的同时实现了高效率和高可扩展性。对于颈部模块，选择了简洁的FPN-LSS[9, 25]。它将细粒度特征与直接上采样的粗粒度特征集成。实际上，由于所提出的范式不受限于特定架构，骨干网络可以替换为其他先进模型，如SwinTransformer[18]、Vit[5]。颈部模块也可以替换为其他竞争性变体，如NAS-FPN[7]、BiFPN[29]。</p><h3 id="_3-2-视图变换器" tabindex="-1"><a class="header-anchor" href="#_3-2-视图变换器"><span>3.2 视图变换器</span></a></h3><p>视图变换器是环绕视角3D感知系统中的关键组件，它将2D感知视图特征映射到BEV表示。</p><p>Lift-splat-shot (LSS)[9, 25]和激光雷达结构(LS)[13]在最近的工作中被广泛使用。LSS利用像素级密集深度预测和相机内外参数将图像特征投影到预定义的3D网格体素上。随后，沿垂直维度（高度）应用池化操作以获得扁平化的BEV表示。然而，LS依赖于均匀分布深度的假设来传递特征，这会导致特征错位，并随后沿相机射线方向产生误检，尽管计算复杂度有所降低。</p><h3 id="_3-3-bev编码器" tabindex="-1"><a class="header-anchor" href="#_3-3-bev编码器"><span>3.3 BEV编码器</span></a></h3><p>BEV编码器通过视图变换增强粗糙的BEV特征，从而生成更详细的3D表示。BEV编码器的架构类似于图像编码器，包括骨干和颈部。我们采用第3.1节中概述的设置。中心特征缺失[6]（对于LSS）或混叠伪影（对于LS）的问题通过骨干中几个块后的特征扩散得到改善。如图2所示，两个多尺度特征被集成以提高表示质量。</p><h3 id="_3-4-占据预测模块" tabindex="-1"><a class="header-anchor" href="#_3-4-占据预测模块"><span>3.4 占据预测模块</span></a></h3><p>如图2所示，从颈部获得的用于占据的BEV特征被输入到占据头。</p><p>它由多层卷积网络[1, 22, 23]或复杂的多尺度特征融合模块[15]组成，后者展现出更优的全局感受野，能够更全面地感知整个场景，同时也提供了对局部细节特征的更精细刻画。</p><p>占据头产生的BEV特征随后通过通道到高度模块。该模块沿通道维度执行简单的重塑操作，将BEV特征从形状B×C×W×H转换为形状B×C*×Z×W×H的占据逻辑，其中B、C、C*、W、H和Z分别表示批量大小、通道数、类别数、3D空间中x/y/z维度的数量，且C = C* × Z。</p><h3 id="_3-5-时间融合模块" tabindex="-1"><a class="header-anchor" href="#_3-5-时间融合模块"><span>3.5 时间融合模块</span></a></h3><p>时间融合模块旨在通过整合历史信息增强对动态物体或属性的感知。它由两个主要组件组成：时空对齐模块和特征融合模块，如图2所示。对齐模块利用自车信息将历史BEV特征与当前激光雷达系统对齐。此对齐过程确保历史特征被正确插值并与当前感知系统同步。对齐完成后，对齐的BEV特征被传递到特征融合模块。该模块结合对齐特征及其时间上下文，生成动态物体或属性的全面表示。融合过程结合了历史特征和当前感知输入的相关信息，以提高整体感知准确性和可靠性。</p><h2 id="_4-实验" tabindex="-1"><a class="header-anchor" href="#_4-实验"><span>4 实验</span></a></h2><p>在本节中，我们首先在第4.1节详细介绍基准测试和指标，以及FlashOcc的训练细节。然后，第4.2节展示了我们的FlashOcc与其他最先进方法在占据预测上的公平比较的主要结果。之后，我们在第4.3节进行了广泛的消融实验，以研究我们提出的FlashOcc中每个组件的有效性。</p><h3 id="_4-1-实验设置" tabindex="-1"><a class="header-anchor" href="#_4-1-实验设置"><span>4.1 实验设置</span></a></h3><p><strong>基准测试</strong></p><p>我们在Occ3D-nuScenes[31]数据集上进行了占据预测实验。Occ3D-nuScenes数据集包含700个训练场景和150个验证场景。该数据集覆盖了X和Y轴上-40米到40米、Z轴上-1米到5.4米的空间范围。占据标签使用0.4米×0.4米×0.4米体素的17个类别定义。每个驾驶场景包含20秒以2Hz频率采集的标注感知数据。数据采集车辆配备了一个激光雷达、五个雷达和六个相机，能够全面感知车辆周围环境。至于评估指标，我们报告了所有类别的平均交并比（mIoU）。</p><p><strong>训练细节</strong></p><p>由于我们的FlashOcc设计为即插即用方式，并且在多样化主流体素级占据方法（即BEVDetOcc[1]、UniOcc[23]和FBOcc[15]）上展示了泛化性和效率，因此训练细节严格遵循原始主流体素级占据方法。由于将3D卷积替换为2D卷积时会改变通道数量，表2中展示了相应插件替换的详细架构。在每个实验表的“方法”列中，我们使用“:”将每个插件替换与其对应结构（即M0-8）关联起来。所有模型均使用AdamW优化器[20]进行训练，其中应用了梯度裁剪，学习率为1e-4，总批量大小为64，分布在8个GPU上。BEVDetOcc和UniOcc的总训练周期设置为24，而FBOcc仅训练20个周期。所有实验均未使用类别平衡分组和采样。</p><h3 id="_4-2-与最先进方法的比较" tabindex="-1"><a class="header-anchor" href="#_4-2-与最先进方法的比较"><span>4.2 与最先进方法的比较</span></a></h3><p>我们在BEVDetOcc[1]和UniOcc[23]上评估了我们的插件FlashOcc，并将我们的插件替换与流行的现有方法（即MonoScene[3]、TPVFormer[10]、OccFormer[36]、CTFOcc[31]、RenderOcc[22]和PanoOcc[33]）的性能进行了比较。如表1所示，列出了Occ3D-nuScenes验证数据集上的3D占据预测性能。评估了使用ResNet-101和SwinTransformer-Base的结果。我们的即插即用FlashOcc实现在BEVDetOcc上展示了1.3 mIoU的提升。此外，UniOcc上0.3 mIoU的提升进一步凸显了通道到高度变换在BEV特征中保留体素级信息的能力，因为UniOcc中的渲染监督需要细粒度体积表示。这些结果证明了我们提出的FlashOcc方法的有效性和泛化性。此外，我们的FO(BEVDetOcc)超越了基于变换器的最先进方法PanoOcc 1.1 mIoU，进一步展示了我们方法的优越性能。</p><p>FO(BEVDetOcc)的定性可视化如图3所示，横跨道路的交通信号横杆（由红色虚线指示）和延伸到道路上方的树（由橙色虚线指示）都可以通过我们的FO(BEVDetOcc)有效地体素化，从而展示了高度信息的保留。关于行人的体素描述（由红色椭圆指示），胸部向前突出的体素表示该人手持的移动设备，而腿部后方延伸的体素表示该人拖拽的行李箱。此外，小型交通锥也在我们的预测占据结果中观察到（由橙色实线矩形指示）。这些发现共同强调了我们的FlashOcc在准确捕捉复杂形状方面的卓越能力。</p><h3 id="_4-3-消融研究" tabindex="-1"><a class="header-anchor" href="#_4-3-消融研究"><span>4.3 消融研究</span></a></h3><p>我们进行了消融实验以证明我们插件替换中每个组件的有效性。除非另有说明，所有实验均使用ResNet-50作为骨干网络，输入图像分辨率为704×256。3D空间的空间表示被离散化为200×200×1的网格大小。所有模型均在3D物体检测任务上进行了预训练。</p><p><strong>无需复杂3D卷积计算的高效通道到高度变换</strong></p><p>我们在占据头的输出处应用通道到高度操作，其中2D特征直接重塑为3D占据逻辑。此过程不涉及显式的高度维度表示学习。从直观角度来看，准确的3D占据预测需要在三个维度上进行体素感知表示，涉及复杂的3D计算，如先前研究广泛讨论的[22, 28, 33]。为了确保公平比较，我们选择不带时间模块的BEVDetOcc[1]作为体素级对比方法。如图4所示，我们将LSS沿z轴的网格大小减小为1，并将BEVDetOcc中的3D卷积替换为2D对应部分。此外，在模型输出处插入了通道到高度变换。比较结果如表4所示。我们的M0方法尽管仅导致0.6 mIoU的性能下降，但实现了超过两倍的加速，以210.6 FPS的速度超越了以92.1 FPS运行的基线方法。而我们的M1模块展示了更优的性能，以60.6Hz的更快FPS实现了显著的0.8 mIoU提升，优于3D体素级表示方法。这些结果进一步凸显了我们提出的通道到高度范式的高效部署兼容性，无需计算密集型3D体素级表示处理。</p><p><strong>在多样化方法上的通用FlashOcc</strong></p><p>为了展示我们即插即用FlashOcc的泛化性，我们旨在通过将其应用于流行的基于3D卷积的占据模型（如BEVDetOcc[1]、RenderOcc[22]和FBOcc[15]）上获得令人信服的结果。具体而言，我们将这些模型中的3D卷积替换为2D卷积，并将占据逻辑替换为通过2D卷积获得的BEV级特征的通道到高度变换。如表5所示，我们的FlashOcc在BEVDetOcc上实现了1.7 mIoU的提升，在FBOcc上实现了0.1 mIoU的提升，同时在UniOcc上保持了可比的性能。这些结果进一步验证了我们提出的通道到高度范式的泛化性。</p><p><strong>时间模块的持续改进</strong></p><p>为了验证时间模块的有效性，我们进一步比较了带和不带时间模块的FlashOcc变体。如表5所示，我们的FlashOcc在BEVDetOcc的非时间和时间变体上分别实现了0.8 mIoU和1.7 mIoU的提升。此外，虽然基线方法在引入时间信息时仅实现了4.5 mIoU的提升，但我们的FlashOcc实现了更优的5.4 mIoU提升。对于基线方法UniOcc，我们的FlashOcc在非时间方法上实现了0.5 mIoU的提升。当引入时间信息时，我们观察到了6.1 mIoU的显著提升，这一提升与基线方法中观察到的时间增强一致。对于基线方法FBOcc，我们的FlashOcc在非时间和时间方法上分别实现了2.0 mIoU和0.1 mIoU的提升。此外，在时间方法中，我们观察到了总体2.6 mIoU的提升。然而，我们的FlashOcc中的时间提升不如基线方法显著。这主要是由于我们的非时间方法与基线方法相比实现了大幅提升。总之，我们的FlashOcc在引入时间信息时展示了显著的提升，与非时间方法相比。此外，我们的FlashOcc在带和不带时间模块的配置中均实现了显著提升或可比性能。</p><p><strong>资源消耗分析</strong></p><p>前述段落已验证了FlashOcc在多样化配置中的性能，将进一步分析模型训练和部署期间的资源消耗。遵循表5中的设置，我们提供了每种方法的FPS、推理时长、推理内存消耗和训练时长的详细信息。鉴于我们的插件适用性受限，仅影响BEV编码器和占据头，我们将这两个组件分类为待检查的独立模块。同时，其余组件（即图像编码器和视图变换）构成了一个自包含模块，称为“其他”以进行分析。</p><p>在BEVDetOcc的情况下，使用我们的FlashOcc导致BEV编码器和占据预测头的推理时长显著减少了58.7%，从7.5毫秒降至3.1毫秒。同时，推理内存消耗大幅节省了68.8%，从398 MiB降至124 MiB。对于不带和带时间融合模块的实验设置，训练时长分别从64减少到32和从144减少到84。此外，由于BEVDetOcc中实现的时间方法是立体匹配，“其他”模块在时间配置下运行时表现出明显更长的推理时间。尽管如此，采用通道分组匹配机制导致相对较少的内存开销。在UniOcc上也得到了类似的结论，因为它与BEVDetOcc具有相似的模型结构。然而，UniOcc中渲染监督的引入显著增加了训练时长。</p><h2 id="_5-结论" tabindex="-1"><a class="header-anchor" href="#_5-结论"><span>5 结论</span></a></h2><p>在本文中，我们介绍了一种即插即用的方法FlashOCC，旨在实现快速且内存高效的占据预测。它直接替换体素级占据方法中的3D卷积为2D卷积，并结合通道到高度变换将扁平化的BEV特征重塑为占据逻辑。FlashOCC的有效性和泛化性已在多样化体素级占据预测方法中得到验证。大量实验证明了该方法在精度、时间消耗、内存效率和部署友好性方面优于先前的最先进方法。据我们所知，我们是首个将子像素范式（通道到高度）应用于占据任务并完全避免使用计算密集型3D（可变形）卷积或变换器模块的研究者。可视化结果令人信服地证明了FlashOcc成功保留了高度信息。在未来的工作中，我们将探索将FlashOcc集成到自动驾驶的感知流程中，以实现高效的芯片部署。</p>',57)]))}const o=c(p,[["render",r]]),i=JSON.parse('{"path":"/%E6%84%9F%E7%9F%A5/nfq4b8kf/","title":"FlashOcc","lang":"zh-CN","frontmatter":{"title":"FlashOcc","createTime":"2025/04/24 13:50:11","permalink":"/感知/nfq4b8kf/","tags":["occ"]},"readingTime":{"minutes":19.52,"words":5857},"git":{"updatedTime":1750820984000,"contributors":[{"name":"weiwen","username":"","email":"a1036359215@163.com","commits":1,"avatar":"https://gravatar.com/avatar/cd8e1d2cae5eb43df4bdc241dd3c0611439067bff22550061317525e3b170bab?d=retro"}]},"filePathRelative":"notes/感知/occ/FlashOcc.md","headers":[],"categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"fcfb7c","sort":10007,"name":"感知"},{"id":"44964a","sort":10014,"name":"occ"}]}');export{o as comp,i as data};
