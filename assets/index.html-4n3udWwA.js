import{_ as a,c as r,b as t,o as n}from"./app-IQA0dJD3.js";const i={};function s(p,e){return n(),r("div",null,e[0]||(e[0]=[t('<p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250311151145722-1677109.png" alt="image-20250311151145722"></p><p>DETR：每个物体query表示一个物体，并在transformer解码器中与2D特征进行交互以生成预测;</p><p>DETR3D:物体query预测的3D参考点通过相机参数投影回图像空间，并用于从所有相机视图中采样2D特征,解码器将采样的特征和query作为输入，更新物体query的表示（说到底仍旧是在2D特征中进行交互）</p><div class="hint-container important"><p class="hint-container-title">重要</p><p>问题：</p><ul><li>参考点的预测坐标可能不够准确，导致采样的特征超出物体区域。</li><li>只会收集投影点处的图像特征，这无法从全局视角进行表示学习.</li><li>复杂的特征采样过程会阻碍检测器的实际应用，DETR 3D 稀疏的query 限制了直接使用密集BEV 特征</li></ul><p>作者的目标是将多视角的2D特征转换为3D感知特征，可以直接在3D环境下更新物体查询。</p></div><h2 id="petr" tabindex="-1"><a class="header-anchor" href="#petr"><span>PETR</span></a></h2><p>2022 ECCV</p><p>https://blog.csdn.net/qq_55794606/article/details/141818806</p><p>https://github.com/megvii-research/PETR?tab=readme-ov-file</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/overview.png" alt="img"></p><p>为了实现这一目标，</p><ul><li>首先将由不同视角共享的相机视锥空间离散化为网格坐标。</li><li>然后，通过不同的相机参数将这些坐标转换为 3D 世界空间的坐标。</li><li>接着，通过主干网络中提取出来的2D 图像特征，与 3D 坐标一起，输入到一个简单的 3D 位置编码器中，以生成 3D 位置感知特征。</li><li>3D 位置感知特征将在 transformer 解码器中与物体查询进行交互，更新后的物体查询将进一步用于预测物体类别和 3D 边界框。</li></ul><p>与 DETR3D 相比，提出的 PETR 架构带来了许多优势。</p><ul><li>它保持了原始 DETR [4] 的端到端特性，同时避免了复杂的 2D 到 3D 投影和特征采样。</li><li>在推理时，3D 位置坐标可以以离线方式生成，并作为额外的输入位置嵌入。这使得实际应用相对更简单。</li></ul><h3 id="模型" tabindex="-1"><a class="header-anchor" href="#模型"><span>模型</span></a></h3><h4 id="_3d-position-encoder" tabindex="-1"><a class="header-anchor" href="#_3d-position-encoder"><span>3D Position Encoder</span></a></h4><p>位置编码</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250311115001300.png" alt="image-20250311115001300"></p><ul><li>多视角2D图像特征输入到一个1×1卷积层中以降低维度。</li><li>由3D坐标生成器产生的3D坐标通过一个多层感知机（MLP）网络转换为3D位置嵌入（PE）。</li><li>然后，<strong>将3D位置嵌入与同一视角的2D图像特征相加，生成3D位置感知特征</strong>。</li><li>最后，将3D位置感知特征展平，作为Transformer解码器的关键输入部分。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span></span></span> 表示展平操作。</li></ul><h4 id="decoder" tabindex="-1"><a class="header-anchor" href="#decoder"><span>Decoder</span></a></h4><p>transformer decoder</p><p>略</p><h4 id="query" tabindex="-1"><a class="header-anchor" href="#query"><span>Query</span></a></h4><p><strong>初始化生成</strong>：</p><p>两层 MLP</p><p><strong>数量</strong>：</p><p>根据消融实验，与固定采样的方式相比，与 DETR 类似可学习的 query 效果更好，且 query 越多模型效果越好，但是消耗上升，取平衡 1500 个 query。</p><p><strong>优化</strong>：</p><p>L 层解码器</p><h3 id="实验" tabindex="-1"><a class="header-anchor" href="#实验"><span>实验</span></a></h3><p><strong>All experiments are trained for 24 epochs (2x schedule) on 8 Tesla V100 GPUs with a batch size of 8. No test time</strong><strong>augmentation methods are used during inference.</strong></p><h2 id="petrv2" tabindex="-1"><a class="header-anchor" href="#petrv2"><span>PETRv2</span></a></h2><p>2022</p><p>ECCV 2023</p><p>PETRv2是将PETR扩展到时序模型以及BEV语义分割上。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250311161232627.png" alt="image-20250311161232627"></p><p>本文提出的PETRv2框架示意图。</p><ul><li>从多视角图像中提取2D特征，并按照 PETR 的方式生成 3D 坐标。</li><li>为了实现时间对齐，前一帧t-1的3D坐标首先通过姿态变换（pose transformation）转换到当前帧 t 的坐标系中。</li><li>然后，两帧的2D特征和3D坐标分别被拼接在一起，并输入到特征引导的位置编码器（Feature-guided Position Encoder, FPE）中，以生成用于Transformer解码器的键（key）和值（value）组件。 <ul><li>检测查询（det queries）、分割查询（seg queries）和车道查询（lane queries）分别初始化于不同的空间，并与Transformer解码器中的多视角图像特征进行交互。</li></ul></li><li>最后，更新后的查询被输入到特定任务的头部（task-specific heads）中，以预测3D边界框、鸟瞰图分割图（BEV segmentation map）和3D车道。 <strong>A⃝</strong> 表示从帧t-1到帧t的3D坐标对齐。 <strong>C⃝</strong> 表示沿批量轴（batch axis）的拼接操作。</li></ul><p>不同点如下：</p><h3 id="时域对齐" tabindex="-1"><a class="header-anchor" href="#时域对齐"><span>时域对齐</span></a></h3><blockquote><p><em>PETR已经证明了3D PE在3D感知中的有效性</em></p><p>在实践中发现，通过简单地将前一帧的3D坐标与当前帧对齐，PETR在时域条件下工作良好。</p></blockquote><p>考虑到自运动，前一帧t-1的3D坐标首先通过姿态变换将变换为当前帧t的坐标系。</p><p>然后，将相邻帧的2D特征和3D坐标分别串联在一起，并输入到特征引导的位置编码器（FPE）。</p><h3 id="特征引导的位置编码" tabindex="-1"><a class="header-anchor" href="#特征引导的位置编码"><span>特征引导的位置编码</span></a></h3><p>引入图片特征加入 3D PE 中。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250311161547854.png" alt="image-20250311161547854"></p><h3 id="query-1" tabindex="-1"><a class="header-anchor" href="#query-1"><span>Query</span></a></h3><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250311161613675.png" alt="image-20250311161613675"></p><p>检测查询（det query）定义在整个3D空间中，而分割查询（seg query）初始化在鸟瞰图（BEV）空间下。车道查询（lane query）则以锚点车道的形式定义，每个锚点车道由300个采样点组成。</p><h2 id="steam-petr" tabindex="-1"><a class="header-anchor" href="#steam-petr"><span>STEAM PETR</span></a></h2><p><a href="https://paperswithcode.com/sota/3d-multi-object-tracking-on-nuscenes-camera-1?p=exploring-object-centric-temporal-modeling" target="_blank" rel="noopener noreferrer"><img src="https://camo.githubusercontent.com/75b0c2e4b83661b55e750867fb7c21d51fae462fb6d882d858301ed5e6a9a0f2/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f6578706c6f72696e672d6f626a6563742d63656e747269632d74656d706f72616c2d6d6f64656c696e672f33642d6d756c74692d6f626a6563742d747261636b696e672d6f6e2d6e757363656e65732d63616d6572612d31" alt="PWC"></a> <a href="https://paperswithcode.com/sota/3d-object-detection-on-nuscenes-camera-only?p=exploring-object-centric-temporal-modeling" target="_blank" rel="noopener noreferrer"><img src="https://camo.githubusercontent.com/95acee2156e846083938cedbbfa141529f3847117e67a545b630b7f312af14af/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f6578706c6f72696e672d6f626a6563742d63656e747269632d74656d706f72616c2d6d6f64656c696e672f33642d6f626a6563742d646574656374696f6e2d6f6e2d6e757363656e65732d63616d6572612d6f6e6c79" alt="PWC"></a><a href="https://arxiv.org/abs/2303.11926" target="_blank" rel="noopener noreferrer"><img src="https://camo.githubusercontent.com/d8617043c5be969759376b28adaf0620c3360f34567782a825b13e9945717a7a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d50617065722d253343434f4c4f522533452e737667" alt="arXiv"></a></p><p>ICCV 2023.</p><p>https://github.com/exiawsh/StreamPETR</p><p>https://arxiv.org/pdf/2303.11926</p><p>StreamPETR-Large is the first online multi-view method that achieves comparable performance (62.0 mAP, 67.6 NDS and 65.3 AMOTA) with the baseline of lidar-based method.</p><h3 id="要解决的问题" tabindex="-1"><a class="header-anchor" href="#要解决的问题"><span>要解决的问题</span></a></h3><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250312114207301.png" alt="image-20250312114207301"></p><p>现有的<strong>基于时序的BEV方法分为两类(a,b)，</strong></p><ul><li><p><strong>其中第一类是在BEV特征上进行时序融合，该类方法是将之前帧的BEV特征通过warp操作来转换到当前帧</strong>。但是这种方法如果想对移动的物体建模，就需要很大的感受野；</p></li><li><p><strong>第二类是基于感知的方法，该类方法可以通过稀疏的query来对移动的物体进行建模</strong>，但是由于每次都要查询之前所有的帧，会带来巨大的计算量。</p></li></ul><p><strong>核心贡献</strong>：提出了以 query 为中间表示的对移动对象进行时序建模的 bev 模型</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/framework.png" alt="img"></p><p>内存队列存储历史对象查询。在Propagation Transformer中，最近的对象查询依次与历史查询和当前图像特征交互，以获得时间和空间信息。</p><p>输出查询被进一步用于生成检测结果，并且前K个非背景目标查询被推送到存储器队列中。</p><p>通过存储器队列的循环更新，长期时间信息被逐帧传播。</p><h3 id="history-memory-queue" tabindex="-1"><a class="header-anchor" href="#history-memory-queue"><span>History Memory Queue</span></a></h3><p>Queue的大小是 NxK，N是储存的帧数，K是储存的object的数量。</p><p>采用先入先出（FIFO）的更新机制，一般根据分类得分选取TopK个前景目标的信息进行存储。</p><p>具体而言，每一个内存空间包含对应object的时间间隔 , 语义embedding ,对象中心点 ,速度 v 和姿态矩阵 E。在实验中我们选取 K=256</p><h3 id="propagation-transformer" tabindex="-1"><a class="header-anchor" href="#propagation-transformer"><span>Propagation Transformer</span></a></h3><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250312113304258.png" alt="image-20250312113304258"></p><p>MLN实现了隐式的运动补偿，Hybrid attention则实现RNN式的时序交互。</p><h4 id="mln" tabindex="-1"><a class="header-anchor" href="#mln"><span>MLN</span></a></h4><p>使用运动信息对 layer norm 后对 query 进行加权</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250312113331254.png" alt="image-20250312113331254"></p><h3 id="实验-1" tabindex="-1"><a class="header-anchor" href="#实验-1"><span>实验</span></a></h3><p>推理：RTX 3090 GPU</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250312114713852.png" alt="image-20250312114713852"></p>',76)]))}const c=a(i,[["render",s]]),m=JSON.parse('{"path":"/%E6%84%9F%E7%9F%A5/qidw72m1/","title":"PETR 系列","lang":"zh-CN","frontmatter":{"title":"PETR 系列","createTime":"2025/04/24 14:25:58","permalink":"/感知/qidw72m1/"},"readingTime":{"minutes":6.06,"words":1818},"git":{"updatedTime":1750820984000,"contributors":[{"name":"weiwen","username":"","email":"a1036359215@163.com","commits":1,"avatar":"https://gravatar.com/avatar/cd8e1d2cae5eb43df4bdc241dd3c0611439067bff22550061317525e3b170bab?d=retro"}]},"filePathRelative":"notes/感知/bev/petr.md","headers":[],"categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"fcfb7c","sort":10007,"name":"感知"},{"id":"308f23","sort":10012,"name":"bev"}]}');export{c as comp,m as data};
