import{_ as e,c as t,b as n,a,o as l}from"./app-IQA0dJD3.js";const i={};function m(r,s){return l(),t("div",null,s[0]||(s[0]=[n('<div class="hint-container warning"><p class="hint-container-title">注意</p><p><a href="https://paperswithcode.com/paper/segment-anything-model-for-medical-image" target="_blank" rel="noopener noreferrer">Segment Anything Model for Medical Image Analysis: an Experimental Study | Papers With Code</a></p><p>SAM outperforms similar methods RITM, SimpleClick, and FocalClick in almost all single-point prompt settings.</p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20231015145710836.png" alt="image-20231015145710836" style="zoom:50%;"></div><h2 id="deep-interactive-object-selection-cvpr-2016" tabindex="-1"><a class="header-anchor" href="#deep-interactive-object-selection-cvpr-2016"><span>Deep Interactive Object Selection. CVPR 2016</span></a></h2><p>Deep Interactive Object Selection 还用了 <strong>graph cut</strong>，处理FCN得到的概率图，得到最终的分割结果</p><p>该篇为将深度学习引入交互式分割的开山之作，贡献在于搭建了点击式交互式分割的基本pipeline，以及train/val protocol, 这些规范都沿用至今, 对后面的文章产生了深远影响。 如下图，它将positive、negative点击用distance map进行表示，和原图concat成5-channel input送进分割模型，预测目标掩码。</p><blockquote><p>提出了为什么语义分割、检测为什么不能直接用于交互式分割的几个原因：</p><p>模型泛化问题、没有见过的实例</p></blockquote><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20221104155527474.png" alt="image-20221104155527474" style="zoom:50%;"><h2 id="interactive-image-segmentation-with-first-click-attention-in-cvpr-2020" tabindex="-1"><a class="header-anchor" href="#interactive-image-segmentation-with-first-click-attention-in-cvpr-2020"><span>Interactive image segmentation with first click attention. In CVPR, 2020.</span></a></h2><p>核心思想是：由于第一个点击一般都会点在目标物体的中心区域，所以第一个点击提供的信息应该比其他的点击多。于是该篇文章使用第一个点击生成attention对feature进行加权，也达到了更好的分割结果。</p><h2 id="interactive-image-segmentation-via-backpropagating-refinement-scheme-cvpr-2019" tabindex="-1"><a class="header-anchor" href="#interactive-image-segmentation-via-backpropagating-refinement-scheme-cvpr-2019"><span>Interactive image segmentation via backpropagating refinement scheme. CVPR, 2019.</span></a></h2><blockquote><p><a href="https://blog.csdn.net/JYZhang_CVML/article/details/100541490?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-100541490-blog-111571066.pc_relevant_aa2&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-100541490-blog-111571066.pc_relevant_aa2&amp;utm_relevant_index=3" target="_blank" rel="noopener noreferrer">Backpropagating Refinement Scheme for Interactive Segmentation 反向传播修正机制 (CVPR2019)_cvpr 2019</a></p></blockquote><p>传统的基于深度学习的交互式分割框架利用<strong>前向传播</strong>得到的结果，还是存在一定的偏差（比如交互部位在最后的分割结果中依然会被分割错）。因此提出 <strong>backpropagating refinement scheme</strong> 进行修正。</p><p>Training phase 按照传统的基于深度学习的交互式分割框架训练模型。 Testing phase</p><ol><li>首先将待分割的图像和用户标注作为输入传入分割框架中进行前向传播得到初始分割结果。</li><li>由于初始前向传播的分割结果可能存在于用户标注不匹配的部分，因此采用反向传播修正机制 修正 interaction map (而不是对模型进行 fine-tuning)，强制结果在用户交互的部分具有正确的结果。</li><li>上述步骤1和步骤2交替进行，直到结果可以满意。 <img src="https://raw.githubusercontent.com/Overmind7/images/main/img/20190904170440647.png" alt="在这里插入图片描述"></li></ol><h2 id="f-brs-rethinking-backpropagating-refinement-for-interactive-segmentation-cvpr-2020" tabindex="-1"><a class="header-anchor" href="#f-brs-rethinking-backpropagating-refinement-for-interactive-segmentation-cvpr-2020"><span>f-brs: Rethinking backpropagating refinement for interactive segmentation. CVPR, 2020.</span></a></h2><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20231015144751741.png" alt="image-20231015144751741"></p><p>这两篇文章的的探索方向相同，核心思想都是:</p><ul><li>用户施加的 postive/negative click都是有前景/背景label的，这些点击区域相当于有ground truth。</li><li>所以这两篇文章利用点击点的label信息对模型参数进行在线微调。</li><li>由此，模型对于特定的图片和特定的点击都会进行case by case的参数更新，从而达到更好的分割效果。</li></ul><div class="hint-container tip"><p class="hint-container-title">Iteratively trained interactive segmentation</p><p>[<a href="https://arxiv.org/abs/1805.04398" target="_blank" rel="noopener noreferrer">1805.04398] Iteratively Trained Interactive Segmentation (arxiv.org)</a></p><p>新的迭代训练策略</p><p>增加了一个mask作为每次迭代的输入</p></div><div class="hint-container tip"><p class="hint-container-title">Continuous Adaptation for Interactive Object Segmentation by Learning from Corrections. ECCV2020</p><p>https://arxiv.org/abs/1911.12709</p><p>非冻结的网络结构，每次推理都是对网络的微调</p></div><h2 id="edgeflow-iccv2021" tabindex="-1"><a class="header-anchor" href="#edgeflow-iccv2021"><span>EdgeFlow. ICCV2021</span></a></h2><p>[<a href="https://arxiv.org/abs/2109.09406" target="_blank" rel="noopener noreferrer">2109.09406] EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow (arxiv.org)</a></p><p><a href="https://blog.csdn.net/qq_53527856/article/details/123899779" target="_blank" rel="noopener noreferrer">(13条消息) EdgeFlow(ICCV2021) 论文阅读笔记（理论篇）_QianZ423的博客-CSDN博客</a></p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/97e2dc43-ab5a-4cd3-82d8-5f7706ad3fe4.png" alt="img"></p><ul><li><p>利用 HRNet-18 作为主干和 OCRNet 作为分割头</p></li><li><p>早晚融合：</p><ul><li>在骨干网络之前融合了交互和图像特征，这就是所谓的早期融合。早期的融合方法普遍存在交互信息提取不正确的问题。交互特征比图像特征稀疏得多，并且包含高级信息，例如 位置信息。骨干网的早期层侧重于低层特征提取，因此交互特征会通过早期层被稀释，网络无法及时响应用户点击。</li><li>为了防止特征稀释，作者提出了一种早晚融合策略来整合交互和图像特征。作者设计了多阶段特征融合，而不是仅在网络开始时融合特征</li></ul></li><li><p>边缘掩码方案，</p><ul><li>全掩码可能会使模型陷入局部最优</li><li>该方案将前一次迭代估计的对象边缘作为先验信息，而不是直接掩码估计。边缘估计比输入的全掩码更稀疏且波动更小，因此可以提高分割的稳定性和效率。</li><li>在交互式分割模型中，交互图像和边缘掩模特征是异构的，导致空间偏差很大。因此，有必要正确对齐这些。光流方法最初用于对齐视频中两个相邻帧的特征。在语义分割中，它对多尺度特征对齐同时融合不同层是有效的。受的启发，作者采用流模块来对齐图像和交互特征，以便可以精确表示空间信息。</li></ul></li></ul><h2 id="ritm" tabindex="-1"><a class="header-anchor" href="#ritm"><span>RITM</span></a></h2><blockquote><p><a href="https://blog.csdn.net/qq_35756383/article/details/114885638" target="_blank" rel="noopener noreferrer">论文阅读 Reviving Iterative Training with Mask Guidance for Interactive Segmentation_络小绎的博客-CSDN博客_reviving iterative training with mask guidance for</a></p><p><a href="https://blog.csdn.net/qq_35756383/article/details/114885638" target="_blank" rel="noopener noreferrer">【论文阅读】Reviving Iterative Training with Mask Guidance for Interactive Segmentation-CSDN博客</a></p><p>《Reviving Iterative Training with Mask Guidance for Interactive Segmentation》论文笔记_m_buddy的博客-CSDN博客](https://blog.csdn.net/m_buddy/article/details/115285213?spm=1001.2101.3001.6650.4&amp;utm_medium=distribute.pc_relevant.none-task-blog-2<sub>default</sub>CTRLIST<sub>Rate-4-115285213-blog-114885638.pc_relevant_3mothn_strategy_and_data_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2</sub>default<sub>CTRLIST</sub>Rate-4-115285213-blog-114885638.pc_relevant_3mothn_strategy_and_data_recovery&amp;utm_relevant_index=6)</p></blockquote><p>核心思想：</p><blockquote><p>修修补补</p></blockquote><p>用户每一次点击，模型都会有一个mask预测的结果，它也可以提供很多信息。由此，该文章在每一次点击之后将上一次点击预测的mask和连同click map, image一起concat成6-channel input输入模型。同时该文章还在loss, 以及模型结构，训练数据等细节上进行了很多探究。 相比与前面的文章，该论文取得了较大的提升。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20231015142900120.png" alt="image-20231015142900120"></p><h2 id="focalclick-towards-practical-interactive-image-segmentation-cvpr2022" tabindex="-1"><a class="header-anchor" href="#focalclick-towards-practical-interactive-image-segmentation-cvpr2022"><span>FocalClick: Towards Practical Interactive Image Segmentation (CVPR2022)</span></a></h2><p>核心思想：</p><ol><li>之前的方法太重了：轻量化的标注工具不希望用又慢又大的模型，如果标注人员达到一定规模。不可能给每个标注员配GPU。</li><li>之前的方法没有兼容性：由于每次需要将所有的点击作为输入，对mask的每个像素进行新的预测，之前的方法根本不能和其他工具配合。举个例子，对人像进行精细分割标注时， 头发的细节需要用matting工具或者手工标注。当切换到别的工具标好了头发后，就无法切回交互式分割工具了。因为一旦用交互式分割工具打一个点，之前标好的头发区域也会被完全重新预测，相当于白标。同理，之前的方法也不能在已有的略带瑕疵的掩膜上进行修改。</li></ol><p>我们发现，问题就出在“每次点击后，模型会对<strong>所有像素全局重新预测</strong>”上。其实每一次点击都是有“小目标”的，如下图，这个点击在球拍上的new click的目标就是把球拍加入前景。由此，我们不需要对球员的区域进行重新预测和更改，只需要对球拍区域进行预测就好了，球员区域的预测保留。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20231015141841170.png" alt="image-20231015141841170"></p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/image-20231015142228103.png" alt="image-20231015142228103"></p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>新的子任务--interactive mask correction</p></div><div class="hint-container tip"><p class="hint-container-title">提示</p><p><a href="https://ai.baidu.com/forum/topic/show/990448" target="_blank" rel="noopener noreferrer">EISeg正式开源-首个高性能交互式自动标注工具 (baidu.com)</a></p><p>基于 EdgeFlow 和 RITM</p></div><h2 id="interactive-object-segmentation-in-3d-point-clouds" tabindex="-1"><a class="header-anchor" href="#interactive-object-segmentation-in-3d-point-clouds"><span>Interactive Object Segmentation in 3D Point Clouds</span></a></h2><p>额外两个通道</p>',40),a("p",{class:"katex-block"},[a("span",{class:"katex-display"},[a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[a("semantics",null,[a("mrow",null,[a("msub",null,[a("mi",null,"T"),a("mi",null,"p")]),a("mo",{stretchy:"false"},"("),a("mi",null,"p"),a("mo",{stretchy:"false"},")"),a("mo",null,"="),a("mrow",null,[a("mo",{fence:"true"},"{"),a("mtable",{rowspacing:"0.16em",columnalign:"center",columnspacing:"1em"},[a("mtr",null,[a("mtd",null,[a("mstyle",{scriptlevel:"0",displaystyle:"false"},[a("mrow",null,[a("mn",null,"1"),a("mo",{separator:"true"},","),a("mtext",null," "),a("mi",null,"i"),a("mi",null,"f"),a("mtext",null," "),a("mi",{mathvariant:"normal"},"∣"),a("msub",null,[a("mi",null,"x"),a("mi",null,"p")]),a("mo",null,"−"),a("msub",null,[a("mi",null,"x"),a("mi",null,"q")]),a("mi",{mathvariant:"normal"},"∣"),a("mo",null,"≤"),a("mi",null,"e"),a("mi",null,"d"),a("mi",null,"g"),a("mi",null,"e"),a("mtext",null," "),a("mi",null,"a"),a("mi",null,"n"),a("mi",null,"d"),a("mtext",null," "),a("mi",{mathvariant:"normal"},"∣"),a("msub",null,[a("mi",null,"y"),a("mi",null,"p")]),a("mo",null,"−"),a("msub",null,[a("mi",null,"y"),a("mi",null,"q")]),a("mi",{mathvariant:"normal"},"∣"),a("mo",null,"≤"),a("mi",null,"e"),a("mi",null,"d"),a("mi",null,"g"),a("mi",null,"e"),a("mtext",null," "),a("mi",null,"a"),a("mi",null,"n"),a("mi",null,"d"),a("mtext",null," "),a("mi",{mathvariant:"normal"},"∣"),a("msub",null,[a("mi",null,"z"),a("mi",null,"p")]),a("mo",null,"−"),a("msub",null,[a("mi",null,"z"),a("mi",null,"q")]),a("mi",{mathvariant:"normal"},"∣"),a("mo",null,"≤"),a("mi",null,"e"),a("mi",null,"d"),a("mi",null,"g"),a("mi",null,"e")])])])]),a("mtr",null,[a("mtd",null,[a("mstyle",{scriptlevel:"0",displaystyle:"false"},[a("mrow",null,[a("mn",null,"0"),a("mo",{separator:"true"},","),a("mtext",null," "),a("mi",null,"o"),a("mi",null,"t"),a("mi",null,"h"),a("mi",null,"e"),a("mi",null,"r"),a("mi",null,"w"),a("mi",null,"i"),a("mi",null,"s"),a("mi",null,"e")])])])])])])]),a("annotation",{encoding:"application/x-tex"},"T_p(p)= \\left\\{\\begin{matrix} 1, ~ if~|x_p-x_q| \\le edge ~and~|y_p-y_q|\\le edge~and ~|z_p-z_q|\\le edge \\\\ 0,~ otherwise \\end{matrix}\\right. ")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1.0361em","vertical-align":"-0.2861em"}}),a("span",{class:"mord"},[a("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"T"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t vlist-t2"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.1514em"}},[a("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mathnormal mtight"},"p")])])]),a("span",{class:"vlist-s"},"​")]),a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.2861em"}},[a("span")])])])])]),a("span",{class:"mopen"},"("),a("span",{class:"mord mathnormal"},"p"),a("span",{class:"mclose"},")"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"="),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"2.4em","vertical-align":"-0.95em"}}),a("span",{class:"minner"},[a("span",{class:"mopen delimcenter",style:{top:"0em"}},[a("span",{class:"delimsizing size3"},"{")]),a("span",{class:"mord"},[a("span",{class:"mtable"},[a("span",{class:"col-align-c"},[a("span",{class:"vlist-t vlist-t2"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"1.45em"}},[a("span",{style:{top:"-3.61em"}},[a("span",{class:"pstrut",style:{height:"3em"}}),a("span",{class:"mord"},[a("span",{class:"mord"},"1"),a("span",{class:"mpunct"},","),a("span",{class:"mspace nobreak"}," "),a("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),a("span",{class:"mord mathnormal"},"i"),a("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),a("span",{class:"mspace nobreak"}," "),a("span",{class:"mord"},"∣"),a("span",{class:"mord"},[a("span",{class:"mord mathnormal"},"x"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t vlist-t2"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.1514em"}},[a("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mathnormal mtight"},"p")])])]),a("span",{class:"vlist-s"},"​")]),a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.2861em"}},[a("span")])])])])]),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),a("span",{class:"mbin"},"−"),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),a("span",{class:"mord"},[a("span",{class:"mord mathnormal"},"x"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t vlist-t2"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.1514em"}},[a("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),a("span",{class:"vlist-s"},"​")]),a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.2861em"}},[a("span")])])])])]),a("span",{class:"mord"},"∣"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"≤"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mord mathnormal"},"e"),a("span",{class:"mord mathnormal"},"d"),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),a("span",{class:"mord mathnormal"},"e"),a("span",{class:"mspace nobreak"}," "),a("span",{class:"mord mathnormal"},"an"),a("span",{class:"mord mathnormal"},"d"),a("span",{class:"mspace nobreak"}," "),a("span",{class:"mord"},"∣"),a("span",{class:"mord"},[a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t vlist-t2"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.1514em"}},[a("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mathnormal mtight"},"p")])])]),a("span",{class:"vlist-s"},"​")]),a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.2861em"}},[a("span")])])])])]),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),a("span",{class:"mbin"},"−"),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),a("span",{class:"mord"},[a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t vlist-t2"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.1514em"}},[a("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),a("span",{class:"vlist-s"},"​")]),a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.2861em"}},[a("span")])])])])]),a("span",{class:"mord"},"∣"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"≤"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mord mathnormal"},"e"),a("span",{class:"mord mathnormal"},"d"),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),a("span",{class:"mord mathnormal"},"e"),a("span",{class:"mspace nobreak"}," "),a("span",{class:"mord mathnormal"},"an"),a("span",{class:"mord mathnormal"},"d"),a("span",{class:"mspace nobreak"}," "),a("span",{class:"mord"},"∣"),a("span",{class:"mord"},[a("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t vlist-t2"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.1514em"}},[a("span",{style:{top:"-2.55em","margin-left":"-0.044em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mathnormal mtight"},"p")])])]),a("span",{class:"vlist-s"},"​")]),a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.2861em"}},[a("span")])])])])]),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),a("span",{class:"mbin"},"−"),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),a("span",{class:"mord"},[a("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t vlist-t2"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.1514em"}},[a("span",{style:{top:"-2.55em","margin-left":"-0.044em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),a("span",{class:"vlist-s"},"​")]),a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.2861em"}},[a("span")])])])])]),a("span",{class:"mord"},"∣"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"≤"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mord mathnormal"},"e"),a("span",{class:"mord mathnormal"},"d"),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),a("span",{class:"mord mathnormal"},"e")])]),a("span",{style:{top:"-2.41em"}},[a("span",{class:"pstrut",style:{height:"3em"}}),a("span",{class:"mord"},[a("span",{class:"mord"},"0"),a("span",{class:"mpunct"},","),a("span",{class:"mspace nobreak"}," "),a("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),a("span",{class:"mord mathnormal"},"o"),a("span",{class:"mord mathnormal"},"t"),a("span",{class:"mord mathnormal"},"h"),a("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"er"),a("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),a("span",{class:"mord mathnormal"},"i"),a("span",{class:"mord mathnormal"},"se")])])]),a("span",{class:"vlist-s"},"​")]),a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.95em"}},[a("span")])])])])])]),a("span",{class:"mclose nulldelimiter"})])])])])])],-1),a("p",null,[a("img",{src:"https://raw.githubusercontent.com/Overmind7/images/main/img/image-20231015144845281.png",alt:"image-20231015144845281"})],-1),a("img",{src:"https://raw.githubusercontent.com/Overmind7/images/main/img/image-20221109161957780.png",alt:"image-20221109161957780",style:{zoom:"67%"}},null,-1)]))}const c=e(i,[["render",m]]),o=JSON.parse('{"path":"/article/ae45vjba/","title":"交互式分割","lang":"zh-CN","frontmatter":{"sidebar":"auto","title":"交互式分割","createTime":"2024/08/11 11:31:15","permalink":"/article/ae45vjba/","tags":["交互式分割"]},"readingTime":{"minutes":6.67,"words":2000},"git":{"updatedTime":1750820984000,"contributors":[{"name":"weiwen","username":"","email":"a1036359215@163.com","commits":1,"avatar":"https://gravatar.com/avatar/cd8e1d2cae5eb43df4bdc241dd3c0611439067bff22550061317525e3b170bab?d=retro"}]},"filePathRelative":"待整理/交互式分割.md","headers":[],"categoryList":[{"id":"e72391","sort":10003,"name":"待整理"}]}');export{c as comp,o as data};
