<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.21" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.143" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><link rel="icon" type="image/png" href="https://theme-plume.vuejs.press/favicon-32x32.png"><title>VLN综述 | aurora</title><meta name="description" content=""><link rel="preload" href="/archive2025/assets/style-Nqw6QQQ5.css" as="style"><link rel="stylesheet" href="/archive2025/assets/style-Nqw6QQQ5.css"><link rel="modulepreload" href="/archive2025/assets/app-IQA0dJD3.js"><link rel="modulepreload" href="/archive2025/assets/index.html-Cwaicfd_.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-d90a7a26><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d5a8d0bc></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-d5a8d0bc> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-d90a7a26 data-v-e98a6132><div class="vp-navbar" vp-navbar data-v-e98a6132 data-v-2c31ea5e><div class="wrapper" data-v-2c31ea5e><div class="container" data-v-2c31ea5e><div class="title" data-v-2c31ea5e><div class="vp-navbar-title has-sidebar" data-v-2c31ea5e data-v-1a4f50af><a class="vp-link no-icon link title" href="/archive2025/" data-v-1a4f50af data-v-442a52aa><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="https://theme-plume.vuejs.press/plume.png" alt data-v-eda4b9bd><!--]--><!--[--><img class="vp-image light logo" style="" src="https://theme-plume.vuejs.press/plume.png" alt data-v-eda4b9bd><!--]--><!--]--><!--]--><span data-v-1a4f50af>aurora</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-2c31ea5e><div class="content-body" data-v-2c31ea5e><!--[--><!--]--><div class="vp-navbar-search search" data-v-2c31ea5e><div class="search-wrapper" data-v-97535d1e><!----><div id="local-search" data-v-97535d1e><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-97535d1e><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-2c31ea5e data-v-d43c1732><span id="main-nav-aria-label" class="visually-hidden" data-v-d43c1732>Main Navigation</span><!--[--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>首页</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>博客</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/tags/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>标签</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link no-icon link navbar-menu-link" href="/archive2025/blog/archives/" tabindex="0" data-v-d43c1732 data-v-d4acf911 data-v-442a52aa><!--[--><!----><span data-v-d4acf911>归档</span><!----><!--]--><!----></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-d43c1732 data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-86530b6c><span class="text" data-v-86530b6c><!----><!----><span data-v-86530b6c>笔记</span><!----><span class="vpi-chevron-down text-icon" data-v-86530b6c></span></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><div class="items" data-v-709dc2b1><!--[--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/demo/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 示例 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/SLAM/zc7d97ub/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> SLAM <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-group" data-v-709dc2b1 data-v-c497e9e3><p class="title" data-v-c497e9e3><!----><span data-v-c497e9e3>AI</span></p><!--[--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 生成模型 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E6%84%9F%E7%9F%A5/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 感知 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E6%95%B0%E6%8D%AE%E9%9B%86/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 数据集 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/%E5%88%86%E5%B1%82%E8%AF%AD%E4%B9%89/" data-v-1ff1855f data-v-442a52aa><!--[--><!----> 分层语义 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-c497e9e3 data-v-1ff1855f><a class="vp-link no-icon link" href="/archive2025/VLN/" data-v-1ff1855f data-v-442a52aa><!--[--><!---->  VLN <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-2c31ea5e data-v-a295abf6><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-a295abf6 data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-2c31ea5e data-v-ad52545c data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="/" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-2c31ea5e data-v-652282fd data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-86530b6c><span class="vpi-more-horizontal icon" data-v-86530b6c></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><!----><!--[--><!--[--><!----><div class="group" data-v-652282fd><div class="item appearance" data-v-652282fd><p class="label" data-v-652282fd>外观</p><div class="appearance-action" data-v-652282fd><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-652282fd data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-652282fd><div class="item social-links" data-v-652282fd><div class="vp-social-links social-links-list" data-v-652282fd data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="/" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-2c31ea5e data-v-2b50024d><span class="container" data-v-2b50024d><span class="top" data-v-2b50024d></span><span class="middle" data-v-2b50024d></span><span class="bottom" data-v-2b50024d></span></span></button></div></div></div></div><div class="divider" data-v-2c31ea5e><div class="divider-line" data-v-2c31ea5e></div></div></div><!----></header><div class="vp-local-nav reached-top" data-v-d90a7a26 data-v-3944d8e8><button class="menu" aria-expanded="false" aria-controls="SidebarNav" data-v-3944d8e8><span class="vpi-align-left menu-icon" data-v-3944d8e8></span><span class="menu-text" data-v-3944d8e8>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-3944d8e8 data-v-4114a62c><button data-v-4114a62c>返回顶部</button><!----></div></div><aside class="vp-sidebar" vp-sidebar data-v-d90a7a26 data-v-3c61d592><div class="curtain" data-v-3c61d592></div><nav id="SidebarNav" class="nav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-3c61d592><span id="sidebar-aria-label" class="visually-hidden" data-v-3c61d592> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-473fd05b><section class="vp-sidebar-item sidebar-item level-0 has-active" data-v-473fd05b data-v-979dadab><!----><div data-v-979dadab data-v-979dadab><div class="items" data-v-979dadab><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/VLN/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>VLN</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/VLN/3ec8oyvr/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>VLN综述</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-979dadab data-v-979dadab><div class="item" data-v-979dadab><div class="indicator" data-v-979dadab></div><!----><a class="vp-link no-icon link link" href="/archive2025/VLN/tfxszik8/" data-v-979dadab data-v-442a52aa><!--[--><p class="text" data-v-979dadab><span data-v-979dadab>Semanticmap综述</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><!--]--><!--[--><!--]--></nav></aside><!--[--><div id="VPContent" vp-content class="vp-content has-sidebar" data-v-d90a7a26 data-v-b2beaca7><div class="vp-doc-container has-sidebar has-aside" data-v-b2beaca7 data-v-a703f9d3><!--[--><!--]--><div class="container" data-v-a703f9d3><div class="aside" vp-outline data-v-a703f9d3><div class="aside-curtain" data-v-a703f9d3></div><div class="aside-container" data-v-a703f9d3><div class="aside-content" data-v-a703f9d3><div class="vp-doc-aside" data-v-a703f9d3 data-v-5976474c><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-5976474c data-v-aa56eba0><div class="content" data-v-aa56eba0><div class="outline-marker" data-v-aa56eba0></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-aa56eba0><span data-v-aa56eba0>此页内容</span><span class="vpi-print icon" data-v-aa56eba0></span></div><ul class="root" data-v-aa56eba0 data-v-3e6b023c><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-5976474c></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-a703f9d3><div class="content-container" data-v-a703f9d3><!--[--><!--]--><main class="main" data-v-a703f9d3><nav class="vp-breadcrumb" data-v-a703f9d3 data-v-1ae4ad7a><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-1ae4ad7a><!--[--><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><a class="vp-link no-icon link breadcrumb" href="/archive2025/" property="item" typeof="WebPage" data-v-1ae4ad7a data-v-442a52aa><!--[-->首页<!--]--><!----></a><span class="vpi-chevron-right" data-v-1ae4ad7a></span><meta property="name" content="首页" data-v-1ae4ad7a><meta property="position" content="1" data-v-1ae4ad7a></li><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><a class="vp-link no-icon link breadcrumb current" href="/archive2025/VLN/3ec8oyvr/" property="item" typeof="WebPage" data-v-1ae4ad7a data-v-442a52aa><!--[-->VLN综述<!--]--><!----></a><!----><meta property="name" content="VLN综述" data-v-1ae4ad7a><meta property="position" content="2" data-v-1ae4ad7a></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-27be53cb>VLN综述 <!----></h1><div class="vp-doc-meta" data-v-27be53cb><!--[--><!--]--><p class="reading-time" data-v-27be53cb><span class="vpi-books icon" data-v-27be53cb></span><span data-v-27be53cb>约 10776 字</span><span data-v-27be53cb>大约 36 分钟</span></p><!----><!--[--><!--]--><p class="create-time" data-v-27be53cb><span class="vpi-clock icon" data-v-27be53cb></span><span data-v-27be53cb>2025-08-18</span></p></div><!--]--><!--[--><!--]--><div class="_VLN_3ec8oyvr_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-a703f9d3><!--[--><!--]--><div data-v-a703f9d3><p>自动驾驶视觉-语言-动作模型综述</p><p>Awesome Vision–Language–Action Models for Autonomous Driving</p><p>（原文：arXiv:2506.24044v1[cs.CV] 30 Jun 2025）</p><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>多模态大语言模型（MLLM）的快速进展为视觉-语言-动作（VLA）范式铺平了道路，该范式将视觉感知、自然语言理解和控制统一在一个策略中。自动驾驶研究人员正积极将这些方法迁移到车辆领域。此类模型有望使自动驾驶车辆能够解释高层指令、推理复杂交通场景并自主决策。然而，现有文献仍显分散且迅速扩张。本文首次给出面向自动驾驶的VLA（VLA4AD）全面综述。我们</p><p>（i）形式化近期工作共享的架构基本模块，</p><p>（ii）追溯从早期解释器到以推理为核心的VLA模型的演进，</p><p>（iii）按VLA在自动驾驶领域的进展对比20余个代表性模型。我们还汇总了现有数据集与基准，强调同时衡量驾驶安全性、准确性与解释质量的协议。</p><p>最后，我们详述开放挑战——鲁棒性、实时效率和形式化验证——并勾勒VLA4AD的未来方向。本综述为可解释、社会对齐的自动驾驶车辆研究提供简明而完整的参考。GitHub仓库已公开。</p><h2 id="_1-引言" tabindex="-1"><a class="header-anchor" href="#_1-引言"><span>1 引言</span></a></h2><p>自动驾驶车辆必须同时感知复杂三维场景、理解交通语境并在实时条件下安全行动。经典自动驾驶（AD）栈通过手工级联的感知[49,80,157]、预测[2,45]、规划[43,109]和控制[12,34,61,62,95,123]模块实现这一目标。数十年研究使其在常见条件下可靠，但模块边界脆弱，难以应对需要高层推理或细致人类判断的长尾极端场景。</p><p>视觉-语言模型（VLM）与大语言模型（LLM）等基础模型进展，将强语义先验引入驾驶感知[53,74,100,102,115]。通过将像素与文本对齐，这些模型可解释场景、回答问题或检索传统检测器可能遗漏的语境信息。早期改进已提升对罕见目标的泛化能力，并给出人类可读的解释，例如描述救护车轨迹或证明红灯停车理由。然而，VLM增强栈仍是被动的：它们推理场景，但不决定行动。其语言输出与低层控制耦合松散，可能产生幻觉或误解口语指令。简言之，VLM虽提升可解释性，却留下“行动缺口”。</p><p>近期工作提出更集成的范式：VLA模型在单一策略内融合相机流、自然语言指令和低层执行[17,57,165]。通过将控制器条件于语言token，这些系统可（i）遵循自由形式指令，如“给救护车让行”[105]，（ii）对其内部推理进行言语化以供事后验证[17]，（iii）利用互联网规模语料的常识先验在罕见或不可预见场景中进行推断[18]。早期原型已在仿真和闭环测试中展现提升的安全性与指令保真度，预示着我们称之为“面向自动驾驶的VLA”的新研究前沿。</p><p>若干汇聚趋势凸显了该新兴研究前沿的及时性。首先，nuScenes[7]、Impromptu VLA[18]等PB级多传感器日志提供丰富多模态监督。其次，基础LLM现可通过低秩更新高效适配，而TS-VLM[11]等token缩减设计将在线计算成本降低一个数量级[167]。第三，SimLingo[105]等合成语料和NuInteract[160]等交互数据集使研究人员能在真实部署前对语言条件行为进行压力测试。这些进展催生了从解释性叠加到具备思维链（CoT）推理的以推理为核心的智能体等广泛的VLA架构。</p><p>尽管已有综述涵盖LLM与VLM在自动驾驶中的应用[22,23,32,166]，但尚未涉及AD中迅速出现的VLA范式。为弥合这一差距并整合快速扩张的工作，我们首次呈现VLA4AD综合综述。我们首先澄清关键术语，关联VLA与传统端到端驾驶；然后提炼通用架构模式，编录20余个代表性模型及其支持数据集；还比较训练范式并总结同时评估控制性能与语言保真度的协议；最后概述开放挑战并描绘未来方向。我们强调需要标准化基准与开源工具包以促进可重复性并加速跨模型比较。我们的目标是为视觉、语言与动作如何汇聚以塑造下一代透明、可指令遵循且社会合规的自动驾驶车辆提供一致而前瞻的参考。</p><h2 id="_2-自动驾驶发展历程" tabindex="-1"><a class="header-anchor" href="#_2-自动驾驶发展历程"><span>2 自动驾驶发展历程</span></a></h2><p>自动驾驶技术演进可划分为四大范式：模块化栈、端到端学习、VLM4AD及最新VLA4AD。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250818092243776.png" alt="image-20250818092243776"></p><blockquote><p>图1 自动驾驶范式比较</p><p>(a) 端到端驾驶：实现从感知到控制的直接映射，但缺乏可解释性与泛化能力。</p><p>(b) 面向自动驾驶的视觉-语言模型（VLM4AD）：引入自然语言推理与可解释性，但仍以感知为中心。</p><p>(c) 面向自动驾驶的视觉-语言-动作模型（VLA4AD）：将感知、推理与动作整合于一体，实现可解释且鲁棒的闭环控制。</p></blockquote><h3 id="_2-1-经典模块化流水线" tabindex="-1"><a class="header-anchor" href="#_2-1-经典模块化流水线"><span>2.1 经典模块化流水线</span></a></h3><p>第一代AD系统——以DARPA城市挑战赛车辆为代表——将驾驶任务显式分解为感知、预测、规划、控制等独立模块。手工算法处理激光雷达、毫米波雷达和GPS数据：传统视觉检测器识别目标，有限状态机或图搜索生成路径，PID或MPC控制器执行最终指令[52,66]。该架构因模块化而被工业界广泛采用，每个组件可独立设计、测试和改进。然而，这种严格解耦导致信息碎片化：上游错误未经修正传播，模块间目标不一致阻碍端到端优化[73,83,107]。</p><h3 id="_2-2-端到端自动驾驶" tabindex="-1"><a class="header-anchor" href="#_2-2-端到端自动驾驶"><span>2.2 端到端自动驾驶</span></a></h3><p>模块化设计在感知、预测、规划模块间存在误差传播和信息损失。因此，研究转向更集成的端到端方法。如图1(a)所示，端到端策略将原始传感器流直接映射为控制指令，绕过手工模块化流水线[9,19,20,27,40,54,106,111,112,138,145,149]。</p><p>端到端驾驶本质上是视觉-动作（VA）系统，视觉输入可来自相机或激光雷达，动作输出通常表示为未来轨迹或控制信号。然而，直接将原始传感器输入映射为驾驶动作带来挑战：<mark>规划级数据稀疏，神经网络解空间巨大[13]</mark>。为缓解此问题，早期端到端方法通过集成感知与预测任务引入中间监督[20,111]。</p><p>具体而言，这些方法在统一框架内集成感知、预测和规划模块，主要促进模块间特征级信息流。</p><ul><li>UniAD[44]主要依赖栅格化表示（如语义图、占据图、流图和代价图），计算密集。</li><li>相反，VAD[59,86]采用全矢量化场景表示进行端到端规划，利用实例级结构信息作为规划约束与指导，以更高效率取得良好性能。</li><li>PolarPoint-BEV[28]通过极坐标点编码进一步优化BEV表示，引入基于距离的重要性加权先验，使模型能在不同距离更有效地关注关键目标。</li></ul><p>为建模自车与其他交通参与者交互，</p><ul><li>GenAD[162]和PPAD[16]利用实例级视觉特征，GraphAD[156]将这些特征表示为图节点。</li><li>SparseAD[151]和SparseDrive[117]构建全稀疏架构，实现更高效率和更优规划性能。</li></ul><p>然而，这些方法通常依赖计算昂贵的BEV特征，并阻止下游任务直接从原始传感器输入学习。</p><p>为应对挑战，PARA-Drive[132]和DriveTransformer[56]引入并行流水线架构，为端到端驾驶系统提供更统一可扩展框架，显式建模感知、预测、规划任务间关系。然而，这些范式在极端场景处理与分布外（OOD）场景下仍不足。例如，遇到罕见事件（如车辆在路口抛锚）时，驾驶系统可能无法生成合适轨迹。</p><p>若干方法尝试缓解这些问题。物理定律增强框架引入先验知识[164]，其他方法利用目标函数和规划动作先验优化轨迹[14,35,54,71,72,77,114,127,144]。然而，为轨迹优化设计此类目标函数既昂贵又费力。后续研究通过自监督或弱监督框架减少3D感知任务标注负担[36,68,69]。</p><p>此外，闭环评估显示，数据量超过某阈值后收益递减，且不同场景类型性能差异显著[92,163]。这些发现表明，纯粹数据规模扩展不足以实现L4+。</p><p>总体而言，端到端学习显著缩小了原始传感器输入与控制决策间差距，但仍面临挑战：</p><ol><li>脆弱语义：对罕见或快速演化场景脆弱；</li><li>不透明推理：可解释性有限，阻碍安全审计与验证；</li><li>语言能力受限：限制直观人机交互。</li></ol><h3 id="_2-3-自动驾驶中的vlm" tabindex="-1"><a class="header-anchor" href="#_2-3-自动驾驶中的vlm"><span>2.3 自动驾驶中的VLM</span></a></h3><p>将语言模态引入驾驶任务有望提升自动驾驶系统的推理、可解释性和泛化能力。LLM[120,122]与VLM[1,74,102]通过统一感知与自然语言推理至共享嵌入空间，提供了有前景的解决方案[47]。核心是大规模多模态预训练，赋予模型常识关联（如警笛→让行），这些关联是特定任务标签常缺失的。因此，语言条件VLM策略在零样本泛化至新目标、天气条件及驾驶规范上表现更强。近期工作[29,46,75,84,97,125,126,154]开始将这些模型直接嵌入驾驶循环，如图1所示。</p><p>早期努力如GPT-Driver[87]表明，冻结VLM可处理多视角图像与文本提示，生成轨迹计划或低层控制token，同时产生人类可读的推理。虽然对常识推理和极端场景理解有效，将大基础模型集成至驾驶系统存在若干缺点：空间感知差[37]、数值输出模糊[58]、规划延迟高[119]。此外，LLM/VLM常见幻觉[136]使驾驶系统面临潜在不安全控制。</p><p>后续研究通过若干关键方向解决限制：</p><ul><li>空间理解增强：TOKEN[118]与WKAD[150]使用目标中心token表示，BEVDriver[134]将BEV特征与语言集成以支持3D空间查询和多模态未来预测，Sce2DriveX[159]提出空间关系图建模自车与交通参与者交互，MPDrive[158]利用视觉提示增强空间推理。</li><li>延迟降低：双系统架构[15,24,25,58,81,86,90,91,100,119]将VLM作为中间模块为端到端系统提供反馈或辅助信号，知识蒸馏方法[38,78,96,140]离线将VLM能力迁移至传统系统。</li><li>幻觉缓解：Dilu[131]及其扩展[60]等上下文学习方法利用记忆库存储关键驾驶信息，ReasonPlan[79]生成逐步决策理由，AgentDriver[88]和AgentThink[99]实现工具增强思维链提示以增强推理。</li></ul><p>尽管取得进展，当前方法仍以感知为中心：其生成计划与闭环控制缺乏紧密集成[110]，解释输出无形式化安全保证[164]。此外，如何对齐VLM输出与动作空间仍待解决。</p><h3 id="_2-4-从vlm到自动驾驶vla" tabindex="-1"><a class="header-anchor" href="#_2-4-从vlm到自动驾驶vla"><span>2.4 从VLM到自动驾驶VLA</span></a></h3><p>受具身智能领域进展启发[4,65,76]，在统一框架内对齐视觉、语言与动作成为自动驾驶新趋势。如图1(c)所示，VLA通过显式动作头解决上述缺口，在单一策略内统一感知、推理与控制[85,108,166]。VLA策略受真实驾驶三大需求驱动：</p><ol><li>在罕见与长尾场景下鲁棒推理[55,128]；</li><li>在动态且部分遮挡条件下噪声容忍控制；</li><li>解释自发 high-level 语言指令（如“超过卡车”）[96,141]。</li></ol><p>利用互联网规模视觉与语言数据预训练的基础模型[94]，VLA模型在跨域与基准上展现强泛化能力[8,116,133]。具体而言，现代VLA模型可：</p><ol><li>将自由形式指令基于自车视觉语境落地并生成对应轨迹[30,148]；</li><li>产生思维链（CoT）理由，如DriveCoT与CoT-VLA[23,124]，以增强可解释性；</li><li>超越直接控制token，集成高级规划模块，包括基于扩散头[57]、分层CoT规划器[30]及混合离散-连续控制。</li></ol><p>近期示例系统突出VLA能力广度[17,30,57,153]，示范了新的VLA4AD范式：联合推理视觉、语言与动作，结合文本与轨迹输出、长时记忆、符号安全检查与多模态扩散规划。这些进展标志着从以感知为中心的VLM流水线向具备行动感知、可解释、指令遵循的多模态智能体关键转变，为更安全、更泛化、更人类对齐的自动驾驶铺平道路。</p><h2 id="_3-vla4ad架构范式" tabindex="-1"><a class="header-anchor" href="#_3-vla4ad架构范式"><span>3 VLA4AD架构范式</span></a></h2><p>本节给出VLA4AD基本架构，整合其多模态接口、核心组件与输出，如图2所示。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250818095446190.png" alt="image-20250818095446190"></p><h3 id="_3-1-多模态输入与语言指令" tabindex="-1"><a class="header-anchor" href="#_3-1-多模态输入与语言指令"><span>3.1 多模态输入与语言指令</span></a></h3><p>VLA4AD依赖丰富多模态传感器流和语言输入以捕捉外部环境与驾驶员高层意图。</p><p><mark class="note"><strong>视觉数据</strong></mark>。人类高度依赖视觉输入以驾驶复杂环境，自动驾驶系统亦然。早期方法采用单目前视相机作为标准视觉模态[63,89,146]。随时间推移，为提升空间覆盖与安全性，系统演进至立体相机、多相机设置，最终至全环视系统[8,116]。更丰富视觉输入支持更鲁棒场景理解与多目标推理。<mark>原始图像可直接处理或转换为结构化中间表示，如鸟瞰图（BEV）地图以辅助空间推理[21,139]</mark>。近期工作进一步探索输入分辨率与模型效率间权衡，动态调整粒度以应对实时或长尾场景。</p><p><mark class="note"><strong>其他传感器数据</strong></mark>。除视觉外，自动驾驶车辆日益利用多样传感器模态补充感知以增强空间能力。早期系统集成激光雷达以获取精确3D结构，后结合毫米波雷达以估计速度、IMU以跟踪运动。GPS模块提供全局定位[8,133]。领域亦日益关注本体感知数据，如转向角、油门与加速度，尤其用于行为预测与闭环控制[20,134,142]。从几何到动力学的演进推动了更复杂传感器融合框架研究[5,10,67,129]，旨在创建<mark>统一时空表示</mark>。</p><p><mark class="note"><strong>语言输入</strong></mark>。自然语言输入——如指令、查询与结构化描述——在VLA4AD中日益重要。早期研究聚焦于直接导航指令（如“下个路口左转”、“停在那辆红车后面”）以实现基本指令跟随[96,105]。随系统成熟，出现环境查询，允许用户或代理提出“现在变道安全吗？”、“此地限速是多少？”等问题[51,93]，实现交互式情境感知。进一步发展引入任务级语言规范，如解释交通规则、解析高层目标或理解自然语言表达地图约束[33]。最新努力推进至多轮对话、思维链推理（如思维链提示）[50,119]及工具增强语言接口[41,88,99]，支持更丰富推理形式及与人类决策过程的对齐。</p><p>最后，最新工作开始引入口语作为更自然、具身的输入模态，通过语音驱动接口桥接感知与交互[148,165]。从静态指令到对话驱动、多步推理的演进，反映了更广趋势：不仅用语言命令车辆，更用其实现可解释与协作式自主。</p><h3 id="_3-2-核心架构模块" tabindex="-1"><a class="header-anchor" href="#_3-2-核心架构模块"><span>3.2 核心架构模块</span></a></h3><p>VLA4AD基本架构在统一流水线中集成视觉感知、语言理解与动作生成。</p><p><mark class="note"><strong>视觉编码器</strong></mark>。原始图像与传感器数据通过大型自监督骨干网络（如DINOv2[94]、ConvNeXt-V2[135]或CLIP[102]）转换为潜在表示。许多VLA系统采用BEV投影[119]，其他通过点云编码器（如PointVLA[67]）或体素模块（3D-VLA[161]）引入3D先验。利用语言派生键的多尺度融合在精细空间层面提升定位[101,152]。</p><p><mark class="note"><strong>语言处理器</strong></mark>。自然语言通过预训练解码器（如LLaMA2[121]或GPT式transformer[6]）处理。指令调优变体（如视觉指令调优[74]）与检索增强提示（RAG-Driver[148]）注入领域知识。轻量级微调策略如LoRA[42]实现高效适配。</p><p><mark class="note"><strong>动作解码器</strong></mark>。下游控制通过（i）自回归分词器顺序预测离散动作或轨迹航点[48,64,98,168]，（ii）扩散头在融合嵌入条件下采样连续控制（DiffVLA[57]；Diffusion-VLA[130]），或（iii）GRPO[113]或DPO[103]微调流水线所用流匹配/策略梯度专家[70,137]。分层控制器（如ORION[30]）允许语言规划器向独立低层PID或MPC栈分派子目标草图。</p><h3 id="_3-3-驾驶输出" tabindex="-1"><a class="header-anchor" href="#_3-3-驾驶输出"><span>3.3 驾驶输出</span></a></h3><p>VLA模型输出模态反映其抽象层次与操作目标。输出格式随时间从低层控制命令演进到更高层空间推理与技能条件动作。</p><p><mark class="note"><strong>低层动作</strong></mark>。早期VLA4AD系统通常直接预测原始控制信号如转向角、油门与制动。这些动作常被建模为连续输出或离散动作token，适用于与PID或端到端控制流水线集成[30,98,143,165]。虽然该公式实现细粒度控制，但对小感知误差敏感且缺乏长时域规划能力。</p><p><mark class="note"><strong>轨迹规划</strong></mark>。后续研究转向轨迹或航点级预测，提供更稳定且可解释的中间表示。这些轨迹常以BEV或自车坐标表达，可灵活通过模型预测控制（MPC）或其他下游规划器执行[3,44,57,59,96,155]。该公式允许VLA模型在更长时域内进行推理并更有效地集成多模态语境。</p><p>综上，这些输出格式说明VLA4AD系统不断演进的雄心：不仅驾驶，更要鲁棒、可解释、情境化地驾驶。简言之，典型VLA4AD模型以多模态传感器数据与自然语言输入为语境，产生驾驶决策（不同抽象层次）及，在某些情况下，语言落地解释。</p><h2 id="_4-vla4ad范式进展" tabindex="-1"><a class="header-anchor" href="#_4-vla4ad范式进展"><span>4 VLA4AD范式进展</span></a></h2><p>VLA4AD研究呈现明显波浪式推进，每一波由前代限制与新的跨模态预训练技术驱动。如图3所示，我们追溯四个连续阶段：解释性语言模型、模块化VLA4AD、端到端VLA4AD及以推理为核心的VLA4AD。表1总结了2023-2025年代表性VLA4AD模型，突出其输入模态、语言集成方式、动作输出形式、评估所用数据/环境及其核心贡献。</p><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/2025/image-20250818095710940.png" alt="image-20250818095710940"></p><h3 id="_4-1-pre-vla-语言模型作为解释器" tabindex="-1"><a class="header-anchor" href="#_4-1-pre-vla-语言模型作为解释器"><span>4.1 Pre-VLA：语言模型作为解释器</span></a></h3><p>最早尝试以被动描述角色集成语言以增强可解释性。此阶段典型流水线采用冻结视觉模型（如CLIP[102]）与LLM解码器，以自然语言解释驾驶场景或推荐动作，但不直接输出控制。例如，DriveGPT-4[141]取单前视相机图像，产生文本描述或高层操作标签（“减速”、“左转”）。这些输出帮助解释感知系统所见或意图，提升透明度。然而，实际车辆控制仍由传统模块（PID控制器等）处理，语言仅为叠加而非决策核心部分。</p><p>此外，出现两个问题：</p><ol><li>逐帧生成长描述引入延迟，因视觉编码器每帧处理数千token[168]；</li><li>通用视觉编码器在非驾驶相关细节上浪费精力[150]。研究者通过TS-VLM[11]等优化响应，使用文本引导软注意力池化聚焦关键区域，DynRsl-VLM[167]动态调整输入分辨率以平衡速度与细节。</li></ol><p>这些提升效率，但仍存在语义鸿沟——叙述或标注场景不等同于生成精确转向或制动命令。弥合该鸿沟成为下一步逻辑步骤。</p><h3 id="_4-2-自动驾驶模块化vla模型" tabindex="-1"><a class="header-anchor" href="#_4-2-自动驾驶模块化vla模型"><span>4.2 自动驾驶模块化VLA模型</span></a></h3><p><mark>随着VLA研究进展，语言从被动场景描述演进为模块化架构内的主动规划组件</mark>。<mark class="important">语言输入输出开始直接为规划决策提供信息[58]</mark>。</p><p>例如，</p><ul><li>OpenDriveVLA[165]融合相机与激光雷达输入及文本路线指令（如“在教堂处右转”），生成中间人类可读航点（如“20米处右转，然后直行”），再转换为连续轨迹。该方法通过引入可解释语言中间步骤提升规划过程透明度与灵活性。</li><li>CoVLA-Agent[17]融合视觉与激光雷达token及可选文本提示，使用紧凑MLP将选定动作token（如“左转”）映射至对应轨迹。</li><li>类似地，DriveMoE[143]采用专家混合架构，语言线索用于动态选择子规划器，如“超车专家”或“停走专家”，依据语境。</li></ul><p>多智能体场景下，</p><ul><li><p>LangCoop[33]展示车辆可通过简洁自然语言消息在路口协调，迈向语言使能协作。</p></li><li><p>SafeAuto[153]引入以形式逻辑表达的符号交通规则，验证或否决语言驱动计划，确保生成行为在约束内安全。</p></li><li><p>此外，RAG-Driver[148]引入检索增强规划机制，从记忆库检索相似历史驾驶案例，指导模糊或长尾场景决策。</p></li></ul><p>总体上，这些方法显著缩小语言指令与车辆动作间语义鸿沟，有效将自然语言嵌入规划循环核心。然而，它们常依赖多阶段流水线（感知→语言→规划→控制），引入延迟与模块边界级联失败风险。这些限制促使近期对更统一端到端架构的兴趣，<mark>旨在将感知、语言理解与动作生成集成至单一可微分网络</mark>。</p><h3 id="_4-3-自动驾驶统一端到端vla模型" tabindex="-1"><a class="header-anchor" href="#_4-3-自动驾驶统一端到端vla模型"><span>4.3 自动驾驶统一端到端VLA模型</span></a></h3><p>借助大型多模态基础模型，研究者转向全统一网络，将传感器（及可选文本命令）直接映射至轨迹或控制信号，单前向传递完成。</p><p>典型例子EMMA[50]在Waymo自动驾驶数据上训练大规模VLM，联合执行目标检测与运动规划；模型学习共享表示服务两任务，实现比独立组件更好闭环性能。</p><p>SimLingo[104]、LMDrive[110]及CarLLaVA[105]基于LLaVA在CARLA模拟器中微调以遵循语言指令驾驶，尤其引入“动作想象”技术，模型通过变化语言指令想象给定场景多样结果，从而强制语言命令与结果轨迹紧密耦合。</p><p>其他创新方法包括使用生成视频模型：</p><ul><li>ADriver-I[53]学习潜在世界模型，通过扩散在给定动作条件下预测未来相机帧，从而通过想象动作结果实现规划。</li><li>DiffVLA[57]结合稀疏（航点）与密集（占据网格）扩散预测，以文本场景描述为条件生成轨迹，有效从合理安全机动分布中采样。</li></ul><p>端到端VLA模型在传感器-动作映射上高度反应且有效，但出现新限制：<mark>在长时域推理（提前规划或考虑复杂偶然性）及提供细粒度解释上仍可能困难</mark>。</p><h3 id="_4-4-自动驾驶推理增强vla模型" tabindex="-1"><a class="header-anchor" href="#_4-4-自动驾驶推理增强vla模型"><span>4.4 自动驾驶推理增强VLA模型</span></a></h3><p>最新VLA4AD浪潮超越解释与计划条件，迈向长时域推理、记忆与交互，将VLM/LLM置于控制循环中心。</p><p>ORION[30]耦合transformer“QTFormer”记忆，存储数分钟观测与动作，LLM总结历史并输出下一轨迹及对应自然语言解释。</p><p>Impromptu VLA[18]将CoT与动作对齐，训练于80k极端案例片段，其正确推理步骤已标注，模型先言语化决策路径再行动，实现车辆间零样本协商最佳性能。AutoVLA[168]融合CoT推理与轨迹规划于单一自回归transformer，将连续路径分词为离散驾驶token，实现nuPlan与CARLA闭环成功率领先。</p><p>总体上，这些系统不再仅对传感器输入反应；它们解释、预测并执行长时域推理后再输出动作。它们指向实时口头证明行动的对话式自动驾驶车辆，但带来新挑战：索引城市规模记忆、保持LLM推理在30Hz控制循环内及形式化验证语言条件策略。</p><p>简言之，VLA4AD模型已从将语言用作被动解释工具演进为将其集成为感知与控制主动组件。我们观察到见、说、做间闭环稳步闭合——从解释性感知，到模块化VLA规划，再到具备推理与对话的完全统一流水线。该演进指向自动驾驶车辆作为会话式、协作智能体——不仅安全驾驶，更能交流并与人类对齐。</p><blockquote><p>表 1 代表性 VLA4AD 模型（2023–2025）</p></blockquote><table><thead><tr><th>模型</th><th>年份</th><th>视觉输入</th><th>语言模型</th><th>动作输出</th><th>训练/评估数据来源</th><th></th><th>核心贡献</th></tr></thead><tbody><tr><td><strong>DriveGPT-4</strong> [141]</td><td>2023</td><td>单目前视</td><td>CLIP</td><td>LLaMA-2</td><td>低层控制 (LLC)</td><td>BDD-X</td><td>可解释 LLM，混合微调</td></tr><tr><td><strong>ADriver-I</strong> [53]</td><td>2023</td><td>单目前视</td><td>CLIP-ViT</td><td>Vicuna-1.5</td><td>LLC</td><td>nuScenes + 私有数据</td><td>扩散世界模型，视觉-动作 token</td></tr><tr><td><strong>RAG-Driver</strong> [148]</td><td>2024</td><td>多视角</td><td>CLIP-ViT</td><td>Vicuna-1.5</td><td>LLC</td><td>BDD-X</td><td>检索增强控制，文本理由</td></tr><tr><td><strong>EMMA</strong> [50]</td><td>2024</td><td>多视角 + 车辆状态</td><td>Gemini-VLM</td><td>Gemini</td><td>多任务 (Multi.)</td><td>Waymo 车队数据</td><td>MLLM 主干，多任务输出</td></tr><tr><td><strong>CoVLA-Agent</strong> [17]</td><td>2024</td><td>单目前视 + 车辆状态</td><td>CLIP-ViT</td><td>Vicuna-1.5</td><td>轨迹 (Traj.)</td><td>CoVLA 数据</td><td>文本 + 轨迹输出，自动标注数据</td></tr><tr><td><strong>OpenDriveVLA</strong> [165]</td><td>2025</td><td>多视角</td><td>定制模块</td><td>Qwen-2.5</td><td>LLC + Traj.</td><td>nuScenes</td><td>2-D/3-D 对齐，SOTA 规划器</td></tr><tr><td><strong>ORION</strong> [31]</td><td>2025</td><td>多视角 + 历史帧</td><td>QT-Former</td><td>Vicuna-1.5</td><td>Traj.</td><td>nuScenes + CARLA</td><td>CoT 推理，连续动作</td></tr><tr><td><strong>DriveMoE</strong> [143]</td><td>2025</td><td>多视角</td><td>Paligemma-3B</td><td>—</td><td>LLC</td><td>Bench2Drive</td><td>专家混合 (MoE)，动态路由</td></tr><tr><td><strong>VaViM</strong> [3]</td><td>2025</td><td>视频帧</td><td>LlamaGen</td><td>GPT-2</td><td>Traj.</td><td>BDD100K + CARLA</td><td>视频 token 预训练，视觉到动作</td></tr><tr><td><strong>DiffVLA</strong> [57]</td><td>2025</td><td>多视角 + 车辆状态</td><td>CLIP-ViT</td><td>Vicuna-1.5</td><td>Traj.</td><td>Navsim-v2</td><td>混合扩散，VLM 采样</td></tr><tr><td><strong>LangCoop</strong> [33]</td><td>2025</td><td>单目前视 + V2V</td><td>GPT-4o</td><td>GPT-4o</td><td>LLC</td><td>CARLA</td><td>基于语言的车-车协同，高带宽切入</td></tr><tr><td><strong>SimLingo</strong> [105]</td><td>2025</td><td>多视角</td><td>InternVL2</td><td>Qwen-2</td><td>LLC + Traj.</td><td>CARLA + Bench2Drive</td><td>增强 VLM，动作想象</td></tr><tr><td><strong>SafeAuto</strong> [153]</td><td>2025</td><td>多视角 + 车辆状态</td><td>CLIP-ViT</td><td>Vicuna-1.5</td><td>LLC</td><td>BDD-X + DriveLM</td><td>基于交通规则，PDCE 损失</td></tr><tr><td><strong>Impromptu-VLA</strong> [18]</td><td>2025</td><td>单目前视</td><td>Qwen-2.5-VL</td><td>Qwen-2.5-VL</td><td>Traj.</td><td>Impromptu 数据</td><td>极端案例问答，Neuro-NCAP SOTA</td></tr><tr><td><strong>AutoVLA</strong> [168]</td><td>2025</td><td>多视角 + 车辆状态</td><td>Qwen-2.5-VL</td><td>Qwen-2.5-VL</td><td>LLC + Traj.</td><td>nuScenes + CARLA</td><td>自适应推理，多基准强化微调</td></tr></tbody></table><h2 id="_5-数据集与基准" tabindex="-1"><a class="header-anchor" href="#_5-数据集与基准"><span>5 数据集与基准</span></a></h2><p>我们回顾若干关键数据集与评估套件，总结于表2。</p><blockquote><p>表 2 面向 VLA4AD 的重要数据集与基准</p></blockquote><table><thead><tr><th>名称</th><th>年份</th><th>领域</th><th>规模</th><th>模态</th><th>任务</th></tr></thead><tbody><tr><td><strong>BDD100K / BDD-X</strong> [63,147]</td><td>2018</td><td>真实（美国）</td><td>10 万段视频；7 k 片段</td><td>RGB 视频</td><td>字幕生成、问答</td></tr><tr><td><strong>nuScenes</strong> [7]</td><td>2020</td><td>真实（波士顿/新加坡）</td><td>1 k 场景（20 s，6 相机）</td><td>RGB、LiDAR、Radar</td><td>检测、问答</td></tr><tr><td><strong>Bench2Drive</strong> [55]</td><td>2024</td><td>仿真（CARLA）</td><td>220 条路线；44 种场景</td><td>RGB</td><td>闭环控制</td></tr><tr><td><strong>Reason2Drive</strong> [93]</td><td>2024</td><td>真实（nuSc/Waymo）</td><td>60 万 视频-问答对</td><td>RGB 视频</td><td>思维链式问答</td></tr><tr><td><strong>DriveLM-Data</strong> [115]</td><td>2024</td><td>真实+仿真</td><td>18 k 场景图</td><td>RGB、图结构</td><td>图问答</td></tr><tr><td><strong>Impromptu VLA</strong> [18]</td><td>2025</td><td>真实（多源）</td><td>80 k 片段（30 s）</td><td>RGB 视频、车辆状态</td><td>问答、轨迹</td></tr><tr><td><strong>NuInteract</strong> [160]</td><td>2025</td><td>真实（nuScenes）</td><td>1 k 场景</td><td>RGB、LiDAR</td><td>多轮问答</td></tr><tr><td><strong>DriveAction</strong> [39]</td><td>2025</td><td>真实（车队）</td><td>2.6 k 场景；16.2 k QA 对</td><td>RGB 视频</td><td>高层问答</td></tr></tbody></table><p>BDD100K/BDD-X[63,147]。BDD100K提供10万段美国多样视频；BDD-X子集（约7k片段）添加时间对齐人类理由（如“因行人穿行减速”），为CoVLA-Agent[17]与SafeAuto[153]等模型提供解释真值。</p><p>nuScenes[7]。1000段20秒真实世界片段（波士顿、新加坡），含6相机、激光雷达+毫米波及完整3D标签。虽无语言，但广泛用于VLA4AD研究。</p><p>Bench2Drive[55]。闭环CARLA基准，含44种场景类型（220条路线）与200万帧训练集。指标隔离特定技能（无保护转弯、切入等）；DriveMoE[143]通过专门化专家混合领先排行榜。</p><p>Reason2Drive[93]。60万视频-文本对（nuScenes、Waymo、ONCE），标注CoT QA，涵盖感知→预测→动作。通过一致性指标评估整个推理链逻辑一致性，惩罚不一致多步问答。</p><p>DriveLM-Data[115]。提供nuScenes（18k图）与CARLA（16k）场景图结构QA，强调条件推理。基线TS-VLM[11]达BLEU-45.6但图一致性低，改进空间充足。</p><p>Impromptu VLA[18]。8万30秒片段（约200万帧）来自8个公开集，筛选极端交通场景（密集人群、救护车、恶劣天气）。每片段配对专家轨迹与高层指令，含丰富描述与时间戳QA。提供开放评估服务器；在此语料上训练可测得闭环安全提升。</p><p>NuInteract[160]。扩展nuScenes，含1000多视角场景，含密集描述与多轮3D QA对，紧密关联激光雷达真值。支持多相机VQA与3D推理；DriveMonkey在此训练后在跨视角QA上显著提升。</p><p>DriveAction[39]。用户贡献真实世界基准，含2600驾驶场景与16200视觉语言QA对，带动作级标签。数据集覆盖广泛、真实场景，提供评估协议，以人类偏好驾驶决策为VLA模型评分，填补感知专用基准空白。</p><p>简言之，数据集覆盖VLA4AD研究全谱：BDD-X[63]与nuScenes[7]提供大规模传感器丰富真实数据；Bench2Drive[55]与Impromptu VLA[18]注入安全关键极端案例；Reason2Drive[93]、DriveLM[115]、NuInteract[160]与DriveAction[39]提供结构化语言，支持细粒度推理与人类对齐动作。整合这些互补资产对训练与基准测试下一代VLA4AD至关重要。</p><h2 id="_6-训练与评估策略" tabindex="-1"><a class="header-anchor" href="#_6-训练与评估策略"><span>6 训练与评估策略</span></a></h2><p>构建VLA4AD策略涉及双重目标：（i）学习安全、胜任的驾驶控制器；（ii）保持忠实语言接口。由于驾驶数据昂贵且风险高，多数工作采用预训练→微调流水线：大规模日志行为克隆，随后在仿真中或用基于规则约束细化。以下回顾两大主导范式——模仿学习与强化学习——并强调近期系统如何结合两者。</p><h3 id="_6-1-训练范式" tabindex="-1"><a class="header-anchor" href="#_6-1-训练范式"><span>6.1 训练范式</span></a></h3><p><mark>监督模仿学习（IL）</mark>。IL仍为VLA4AD主力：网络摄入传感器流（如有语言提示）并以l2或交叉熵损失最小化复制专家控制或轨迹。CoVLA-Agent[17]每帧学习未来路径与场景描述，DriveMoE[143]与SimLingo语料（CarLLAVA）[105]训练模型克隆数百万仿真演示。尽管IL易于扩展，但镜像训练分布；罕见危险（如切入、遮挡行人）监督不足。典型补救为DAgger式噪声推出或显式极端案例增广，但分布漂移仍可能级联。</p><p><mark>强化学习（RL）</mark>。RL通常叠加于IL热启动之上。策略与仿真器（CARLA、Bench2Drive等）交互，以PPO或DQN式更新优化路线进度、避碰与交规合规。多智能体设置如LangCoop[33]用RL细化V2V协调，SafeAuto[153]将逻辑交规嵌入为硬约束或附加惩罚。关键开放问题是如何融合驾驶奖励与语言保真：当前工作常冻结LLM，仅惩罚不安全动作，文本与控制联合梯度基本未探索。因此，VLA的RL前景可期——尤其对边缘案例鲁棒性——但相比纯驾驶RL仍待发展。</p><p><mark>多阶段训练</mark>。多数VLA4AD模型通过四阶段课程训练：（1）预训练大型视觉编码器（如CLIP、InternViT）与语言模型（如LLaMA、Vicuna）于广泛图文语料与视频数据集，学习通用视觉与语言先验；（2）在配对图像-文本-动作数据上微调以对齐模态——如DriveMonkey在NuInteract[160]上——使用跨模态对比损失与序列建模目标绑定场景特征、语言提示与控制token；（3）针对性增广注入特殊交通场景与指令（如SimLingo极端案例片段[105]，Bench2Drive挑战路线[55]），通常辅以强化学习或基于规则惩罚（如SafeAuto[153]）以强制安全约束并提升罕见事件性能；（4）通过参数高效方法压缩模型用于部署——LoRA适配器、稀疏专家混合路由或师生蒸馏——降低计算、内存与延迟，同时保持对齐VLA能力，如DriveMoE[143]与TS-VLM[11]所示。</p><p><mark>平衡语言与控制</mark>。联合损失通常权衡轨迹项与描述或QA项（如CoVLA-Agent用L = L_traj + λL_cap）。一些作者交替更新——一批驾驶，一批语言——以避免梯度干扰。大LLM常冻结，仅通过轻适配器提示；仅提示编码器训练，保持语言流畅度且无需大GPU成本。自由形式解释使监督复杂化：CIDEr式或基于强化学习的描述调优已探索，但需奖励事实准确性而非修辞。</p><p><mark>可扩展性与效率</mark>。端到端VLA栈（视觉transformer + LLM + 规划器）每帧可达数百GFLOPs。当前工作依赖：LoRA与适配器在70B LLM内仅更新数百万参数（SafeAuto[153]）；专家混合路由使推理时仅部分专家运行（DriveMoE[143]）；轻量token缩减——TS-VLM通过软注意力池化报告10×加速[167]；事件驱动调度仅在新场景时调用重型模型；蒸馏（文献中仍少见）将集群规模策略压缩为嵌入式“微型VLA”。典型流水线现冻结CLIP与LLaMA，通过模仿训练小交叉注意力头，LoRA在语言增广数据上适配LLM，可选强化学习用于红灯惩罚，最后蒸馏结果用于车载部署。</p><h3 id="_6-2-评估协议" tabindex="-1"><a class="header-anchor" href="#_6-2-评估协议"><span>6.2 评估协议</span></a></h3><p>评估VLA4AD智能体是双重目标任务：策略必须安全驾驶且忠实交流。因此，前沿论文报告四大互补指标支柱：</p><p><mark>闭环驾驶</mark>。CARLA/Bench2Drive[55]路线成功率；违规（碰撞、闯红灯、驶离道路），DiffVla通过PDMS层减半错误[57]；SafeAuto[153]用逻辑否决检查规则合规；泛化至未见城镇与天气。</p><p><mark>开环预测</mark>。nuScenes挑战轨迹l2误差与碰撞率；指令条件目标到达率；可选辅助感知头mAP/IoU；延迟/FPS——TS-VLM通过token池化削减约90%计算[167]。</p><p><mark>语言能力</mark>。SimLingo动作想象基准[105]指令遵循；NuInteract[160]与DriveLM（TS-VLM达BLEU-45.6[115]）自动BLEU/CIDEr/准确率；Reason2Drive[93]推理链一致性；BDD-X风格理由人工评分。</p><p><mark>鲁棒性与压力测试</mark>。DynRsl-VLM[167]分析传感器扰动（模糊、丢帧、延迟）、对抗提示或补丁、分布外事件及语言边缘案例（习语、语码转换、多语言查询）。</p><p>总体而言，可信评估必须衡量（i）控制可靠性，（ii）语言保真度，（iii）二者耦合。当前套件分别覆盖这些方面——CARLA/Bench2Drive用于控制，NuInteract/Reason2Drive/DriveLM用于推理——凸显需要统一“AI驾照”融合双流。</p><h2 id="_7-开放挑战" tabindex="-1"><a class="header-anchor" href="#_7-开放挑战"><span>7 开放挑战</span></a></h2><p>尽管进展迅速，视觉-语言-动作系统在规模化真实部署前仍面临重大障碍。以下总结六大前沿研究问题。</p><p>鲁棒性与可靠性。语言推理增添语境，但引入新失效模式：LLM可能幻觉危险或误解俚语（如“地板油”）。模型需在传感器损坏（雨、雪、眩光）与语言噪声下保持稳定。SafeAuto[153]等基于逻辑的安全否决是第一步，但形式化验证与“社会合规”驾驶策略基本未探索。</p><p>实时性能。在汽车硬件上以≥30Hz运行视觉transformer加LLM非易事。TS-VLM[167]等token缩减设计、硬件感知量化及事件触发推理（重型模块仅在新场景激活）前景可期；蒸馏或专家混合稀疏性将随模型规模扩大至数十亿参数必需。</p><p>数据与标注瓶颈。三模态监督（图像+控制+语言）稀缺且昂贵——Impromptu VLA需8万人工标注片段。合成增广（如SimLingo）有助，但非英语方言、交通俚语及法律约束力措辞覆盖仍薄。</p><p>多模态对齐。当前VLA4AD以相机为中心；激光雷达、毫米波、高精地图与时序状态仅部分融合。方法涵盖从点云BEV投影到3D token适配器，从ORION语言总结长历史[31]到RAG-Driver检索文本地图规则[148]。尚缺原则性、时序一致的多模态融合。</p><p>多智能体社会复杂性。从成对协调扩展至密集交通，带来协议、信任与安全问题。自动驾驶车辆如何在受限且灵活的“交通语言”中交换意图？认证与恶意消息鲁棒性是开放问题；加密V2V与手势-文本落地是早期研究线索。</p><p>领域适应与评估。仿真到真实迁移、跨区域泛化与无灾难性遗忘持续学习未解决。社区基准（如Bench2Drive）仅覆盖长尾子集。监管“AI驾驶测试”评分控制与解释质量仍待定义。</p><p>简言之，应对这些挑战需可扩展训练、形式化安全分析、人机交互与策略联合进展。任一前沿进展——鲁棒感知、高效LLM、可信V2V或标准化评估——将加速迈向安全、透明、全球可部署VLA4AD系统。</p><h2 id="_8-未来方向" tabindex="-1"><a class="header-anchor" href="#_8-未来方向"><span>8 未来方向</span></a></h2><p>下一代研究可能将VLA4AD范围从原型策略扩展至可扩展、协作、可验证驾驶平台。我们突出五大有前景线索。</p><p>基础规模驾驶模型。明显趋势是GPT式“驾驶骨干”：自监督、多传感器模型，基于行车记录仪、激光雷达扫描、高精地图与文本道路规则训练。此类模型可通过提示或LoRA适配下游任务，数据需求少，类似SimLingo/CarLLaVA利用指令条件轨迹[105]。实现该愿景需掩码多模态目标与架构，处理全景视频与自由文本。</p><p>神经符号安全内核。纯端到端网络难保障安全。近期混合体添加规则层——如SafeAuto插入逻辑交通检查[153]。未来工作可让神经VLA栈输出结构化动作程序（或CoT计划），由符号验证器执行，桥接灵活性与可认证性；ORION语言记忆暗示此类接口[31]。</p><p>车队规模持续学习。部署自动驾驶车辆将每日遇新危险。车辆可上传简洁语言片段（如“新旗手模式在x,y”）聚合为课程更新，如SimLingo过滤琐碎场景强调罕见案例[105]。云代理甚至可实时回答不确定车辆查询，跨车队引导知识。</p><p>标准化交通语言。广域协调需受控、本体驱动消息集——“我让给你”、“前方障碍”等——类比航空ICAO用语。VLA模型自然将原始感知翻译为规范意图；专家混合路由（DriveMoE[143]）或token缩减LM（TS-VLM[167]）可保持V2V链路带宽足够低。</p><p>跨模态社会智能。未来系统需解析手势、语音与标志作为“语言”通道一部分——如识别警察手势或行人挥手，再产生显式人类可读响应（灯光、显示屏、喇叭）。RAG-Driver[148]等检索增强规划器提示一条路径：融合实时感知与符号规则及语境以落地非语言线索。扩展至鲁棒手势-语言-动作对齐仍开放。</p><p>简言之，实现这些目标需大规模多模态学习、形式化验证、通信标准与人机交互进展。成功将产生通用“驾驶大脑”，可快速适配、安全审计并无缝融入全球交通生态系统。</p><h2 id="_9-结论" tabindex="-1"><a class="header-anchor" href="#_9-结论"><span>9 结论</span></a></h2><p>本文首次全面综述面向自动驾驶的视觉-语言-动作模型（VLA4AD），在简明分类法下统一若干代表性方法，涵盖输入模态、核心架构组件与输出格式。我们追溯VLA4AD通过四个连续浪潮的演进——前VLA解释器、模块化VLA4AD、端到端VLA4AD及推理增强VLA4AD——突出每阶段如何逐步闭合感知、语言理解与控制间循环。</p><p>我们深入比较训练范式，从大规模预训练与模态对齐到针对性极端案例数据增广与高效压缩技术，说明这些多阶段工作流程如何产生既表达丰富又可部署的模型。我们对数据集与基准的回顾强调了丰富、多传感器、语言落地语料在推进VLA4AD能力中的关键作用。</p><p>尽管进展迅速，重大挑战仍在：确保30Hz内推理吞吐量、语言条件策略形式化验证、长尾场景鲁棒泛化及无缝仿真到真实迁移。我们认为社区必须汇聚共享评估协议与开源工具包，投资可扩展记忆与因果推理骨干，并追求持续、车队规模学习以桥接研究原型与生产系统。通过提炼当前成果并勾勒开放方向，我们旨在激励未来工作，实现由集成视觉、语言与动作驱动的透明、可指令遵循且社会对齐的自动驾驶车辆。</p></div><!----><!----><!----></div></main><footer class="vp-doc-footer" data-v-a703f9d3 data-v-fda6bbae><!--[--><!--]--><!----><!----><nav class="prev-next" data-v-fda6bbae><div class="pager" data-v-fda6bbae><a class="vp-link no-icon link pager-link prev" href="/archive2025/VLN/" data-v-fda6bbae data-v-442a52aa><!--[--><span class="desc" data-v-fda6bbae>上一页</span><span class="title" data-v-fda6bbae>VLN</span><!--]--><!----></a></div><div class="pager" data-v-fda6bbae><a class="vp-link no-icon link pager-link next" href="/archive2025/VLN/tfxszik8/" data-v-fda6bbae data-v-442a52aa><!--[--><span class="desc" data-v-fda6bbae>下一页</span><span class="title" data-v-fda6bbae>Semanticmap综述</span><!--]--><!----></a></div></nav></footer><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-d90a7a26 data-v-bcf8d9a6><span class="percent" data-allow-mismatch data-v-bcf8d9a6>0%</span><span class="show icon vpi-back-to-top" data-v-bcf8d9a6></span><svg aria-hidden="true" data-v-bcf8d9a6><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-bcf8d9a6></circle></svg></button><footer class="vp-footer has-sidebar" vp-footer data-v-d90a7a26 data-v-400675cf><!--[--><div class="container" data-v-400675cf><!----><p class="copyright" data-v-400675cf>wenwei@2025</p></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/archive2025/assets/app-IQA0dJD3.js" defer></script></body></html>